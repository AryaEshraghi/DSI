{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Cross-Validation Lesson\n",
    "\n",
    "_Authors: Dave Yerrington (SF), Joseph Nelson (DC), Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- **Describe** train/test split and cross-validation.\n",
    "- **Explain** how these validation techniques differ and why we want to use them.\n",
    "- **Split** data into testing and training sets using both train/test split and cross-validation and **apply** both techniques to score a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Overfitting and Underfitting](#overfitting-underfitting)\n",
    "- [Load the Data](#demo)\n",
    "- [Train/Test Split and Model Evaluation](#train-test-split)\n",
    "- [K-Fold Cross-Validation Demonstration](#cross-val-k-fold)\n",
    "- [Hold-out Sets](#hold-out)\n",
    "- [Additional Resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfitting-underfitting'></a>\n",
    "\n",
    "## Overfitting and Underfitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://tomrobertshaw.net/img/2015/12/overfitting.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's wrong with the first model?**\n",
    "- The underfit model falls short of capturing the complexity of the \"true model\" of the data.\n",
    "\n",
    "**What's wrong with the third model?**\n",
    "- The overfit model is too complex and is modeling random noise in the data.\n",
    "\n",
    "**The middle model is a good compromise.**\n",
    "- It approximates the complexity of the true model and does not model random noise in our sample as true relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://image.slidesharecdn.com/nncollovcapaldo2013-131220052427-phpapp01/95/machine-learning-introduction-to-neural-networks-12-638.jpg?cb=1393073301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check 1:** Why is overfitting a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "\n",
    "## Load the Data\n",
    "\n",
    "---\n",
    "\n",
    "Let's use scikit-learn to load everyone's favorite data set: the Boston housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Load the Boston Housing data set.\n",
    "boston = datasets.load_boston()\n",
    "boston.keys()\n",
    "\n",
    "#\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "# Take a look at the data again.\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a Heatmap of the Correlation Matrix\n",
    "\n",
    "Heatmaps are an effective way to visually examine the correlational structure of your predictors. \n",
    "\n",
    "> Keep in mind that Pearson correlation between non-dummy-coded categorical variables and other variables is invalid.\n",
    "\n",
    "[Plotting a diagonal correlation matrix](https://seaborn.pydata.org/examples/many_pairwise_correlations.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_heat_map(df):\n",
    "    corrs = df.corr()\n",
    "\n",
    "    # Set the default matplotlib figure size:\n",
    "    fig, ax = plt.subplots(figsize=(11,7))\n",
    "\n",
    "    # Generate a mask for the upper triangle (taken from the Seaborn example gallery):\n",
    "    mask = np.zeros_like(corrs, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Plot the heatmap with Seaborn.\n",
    "    # Assign the matplotlib axis the function returns. This allow us to resize the labels.\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    ax = sns.heatmap(corrs, mask=mask, annot=True, cmap=cmap, vmin=-1, vmax=1)\n",
    "\n",
    "    # Resize the labels.\n",
    "    ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14, rotation=30)\n",
    "    ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14, rotation=0)\n",
    "\n",
    "    # If you put plt.show() at the bottom, it prevents those useless printouts from matplotlib.\n",
    "    plt.show()\n",
    "\n",
    "df_with_target = df.copy()\n",
    "df_with_target['MEDV'] = y\n",
    "correlation_heat_map(df_with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check 2:** When are correlation heatmaps useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='single-predictor'></a>\n",
    "\n",
    "### Select a Single Predictor for an SLR\n",
    "\n",
    "The variable `AGE` appears to have a minor linear relationship with the target variable `MEDV`.\n",
    "\n",
    "Let's select `AGE` out of the data as a single-column design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['AGE']]\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's good practice to plot the variable against the target to confirm the relationship visually.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check 3:** Take 3 minutes to brainstorm ways that you could judge model performance in a way that would _not_ be sensitive to overfitting.\n",
    "\n",
    "> In what case does overfitting hurt you?  In other words, under what circumstance are you able to determine that you have indeed overfit a model?  Can you think of a way to catch that scenario sooner rather than later?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"train-test-split\"></a>\n",
    "## Train/Test Split and Model Validation\n",
    "\n",
    "---\n",
    "\n",
    "So far we've focused on fitting the best model to our data. But is this the best model for our sample data or the best model overall? How do we know?\n",
    "\n",
    "In practice we need to validate our model's ability to generalize to new data. One popular method for performing model validation is by splitting our data into subsets: data on which we *train* our model and data on which we *test* our model.\n",
    "\n",
    "The most basic type of \"hold-out\" validation is called **train/test split**. We split our data into two pieces:\n",
    "\n",
    "> **\"A Training Set\":** The subset of the data on which we fit our model.\n",
    "\n",
    "> **\"A Testing Set\":** The subset of the data on which we evaluate the quality of our predictions.\n",
    "\n",
    "\n",
    "**Train/Test Split Benefits:**\n",
    "\n",
    "- Testing data can represent \"future\" data; for prediction-oriented models, it's critical to ensure that a model that is performing well on current data will likely perform well on future data.\n",
    "- It can help diagnose and avoid overfitting via model tuning.\n",
    "- It can improve the quality of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sklearn-tts'></a>\n",
    "\n",
    "### Scikit-Learn's `train_test_split` Function\n",
    "\n",
    "Performing train/test splits using scikit-learn is easy — load the `train_test_split` function:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "```\n",
    "\n",
    "**Arguments**:\n",
    "- *Arrays*: Any number of arrays/matrices to split up into training and testing sets (they should be the same length).\n",
    "- `test_size`: An integer representing the exact size of the testing subset or a float for a percentage.\n",
    "- `train_size`: Alternatively, you can specify the training size.\n",
    "- `stratify`: Supply a vector to stratify the splitting (by more important classification tasks).\n",
    "\n",
    "**Perform a 50-50 split of our `X` and `y`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the docs: [http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "Note that we could always split the data up manually. Here's an example for of manually split code for [this data set](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fit-on-train'></a>\n",
    "\n",
    "### Fit a Linear Regression on the Training Set\n",
    "\n",
    "Using the training `X` and training `y`, we can fit a linear regression with scikit-learn's `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='score-on-test'></a>\n",
    "\n",
    "### Calculate the $R^2$ Score on the Testing Data\n",
    "\n",
    "After we've constructed our model on the training set, we can evaluate how well it performs on data that it's had no exposure to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare this to how the model scored on the training set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check 4:** What is the benefit of having a testing set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cross-val-k-fold'></a>\n",
    "\n",
    "## K-Fold Cross-Validation\n",
    "\n",
    "---\n",
    "\n",
    "K-fold cross-validation takes the idea of a single train/test split and expands it to *multiple tests* across different train/test splits of your data.\n",
    "\n",
    "For example, if you determine your training set will contain 80 percent of the data and your testing set will contain the other 20 percent, you could have five different 80/20 splits in which the test set in each is a different set of observations. We have:\n",
    "- Five (K=5) training sets.\n",
    "- Five (K=5) corresponding testing sets.\n",
    "\n",
    "**K-fold cross-validation builds K models — one for each train/test pair — and evaluates those models on each respective test set.**\n",
    "\n",
    "### K-Fold Cross-Validation Visually\n",
    "\n",
    "<img src=\"https://snag.gy/o1lLcw.jpg?convert_to_webp=true\" width=\"500\"a>\n",
    "\n",
    "---\n",
    "\n",
    "Cross-validation helps us understand how a model parameterization may perform in a variety of cases. The k-fold cross-validation procedure can be described in pseudocode:\n",
    "\n",
    "```\n",
    "set k\n",
    "create k groups of rows in data\n",
    "\n",
    "for group i in k row groups:\n",
    "    test data is data[group i]\n",
    "    train data is data[all groups not i]\n",
    "    \n",
    "    fit model on train data\n",
    "    \n",
    "    score model on test data\n",
    "    \n",
    "evaluate mean of k model scores\n",
    "evaluate variance of k model scores\n",
    "```\n",
    "\n",
    "Odd case No. 1:\n",
    "> **When K=2**: This is equivalent to performing ***two*** mirror image 50-50 train/test splits.\n",
    "\n",
    "Odd case No. 2:\n",
    "> **When K=number of rows**: This is known as \"leave-one-out cross-validation,\" or LOOCV. A model is built on all but one row and tested on the single excluded observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Folds Cross Validation in `sklearn`\n",
    "\n",
    "Now let's try out k-fold cross-validation. Again, scikit-learn provides useful functions to do the heavy lifting. \n",
    "\n",
    "The function `cross_val_score` returns the $R^2$ for each testing set. \n",
    "\n",
    "Alternatively, the function `cross_val_predict` returns the predicted values for each data point when it's in the testing slice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check 5:** K-Fold Cross Validation Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='neg-r2'></a>\n",
    "## Review: Negative $R^2$ Values\n",
    "\n",
    "---\n",
    "\n",
    "What does it mean to have a negative $R^2$?\n",
    "\n",
    "A negative $R^2$ only makes sense (and can only be found) when we're evaluating the $R^2$ score on data on which the model was not fit. If $R^2$ is evaluated for a model using the training data, *the minimum $R^2$ must be zero.* \n",
    "\n",
    "However, on a testing set the $R^2$ **can** be negative. This means that the model performs so poorly on the testing set that you would have been better off just using the mean of the target from the training set as an estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hold-out'></a>\n",
    "\n",
    "## Hold-Out Sets\n",
    "\n",
    "---\n",
    "\n",
    "Hold-out sets are a version of train/test split. To create a hold-out set, you:\n",
    "\n",
    "1) **Split data into a large training and a small testing set. This small testing set will be the \"hold-out\" set.**\n",
    "\n",
    "2) **For a set of different model parameterizations:**\n",
    "\n",
    "    1) Set up the model.\n",
    "    2) Cross-validate the current model on the training data.\n",
    "    3) Save the model performance.\n",
    "\n",
    "3) **Select the model that performed best using cross-validation on the training data.**\n",
    "\n",
    "4) **Perform a final test of that model on the original \"hold-out\" test set.**\n",
    "\n",
    "> **Note:** The \"hold-out\" method is more conservative, but also requires you to have more data. With smaller data sets it can be infeasible.\n",
    "\n",
    "The graphic below explains the hold-out method visually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "---\n",
    "\n",
    "- [Cross-Validation Example](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py).\n",
    "- [Plotting Cross-Validated Predictions](http://scikit-learn.org/stable/auto_examples/plot_cv_predict.html).\n",
    "- Review this [academic paper](http://frostiebek.free.fr/docs/Machine%20Learning/validation-1.pdf) on the underpinnings of the hold-out method, LOOVC, and k-folds.\n",
    "- Review the scikit-learn [documentation](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation) on cross-validation.\n",
    "- Review this [Stanford lesson](https://www.youtube.com/watch?v=_2ij6eaaSl0) on cross-validation.\n",
    "- Review this [blog post](http://www.win-vector.com/blog/2015/01/random-testtrain-split-is-not-always-enough/) on why TTS is not always sufficient.\n",
    "- Review this Stack Exchange [discussion](http://stackoverflow.com/questions/13610074/is-there-a-rule-of-thumb-for-how-to-divide-a-dataset-into-training-and-validatio) on approximate TTS and validation set sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
