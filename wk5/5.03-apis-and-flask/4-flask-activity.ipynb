{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Practice Building a Production Model\n",
    "Author:  Dave Yerrington (SF)\n",
    "\n",
    "Integrate an endpoint called \"predict-iris\" that will use a classification method of your choice.  Build a model in Jupyter, then load it into Flask and use it!\n",
    "\n",
    "1. Build a response that will react to these parameters:\n",
    "- sepal_len and sepal_width\n",
    "- Have the response return a message that includes class predictions and probabilities (if the model provides)\n",
    "- Have the response return a message to inform your users to set the correct parameters if they are not set\n",
    "\n",
    "2. Extend your model to include the additional 2 parameters to predict on\n",
    "3. Extend your service to accept multiple predictions at once\n",
    "> Check this out for handling multiple input items:\n",
    ">\n",
    "> http://stackoverflow.com/questions/14188451/get-multiple-request-params-of-the-same-name\n",
    "\n",
    "**BONUS**\n",
    "\n",
    "Implement cross-validation in your notebook model and additionally return the recall, precision, and plot a ROC curve.  Save your classification metric results as a new object (for instance a dictionary), serialize using joblib, then load them as part of your response in your Flask service for the iris prediction.\n",
    "\n",
    "\n",
    "See these details:\n",
    "http://scikit-learn.org/stable/modules/model_persistence.html\n",
    "http://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "\n",
    "Here is some reference / starter code to get you going with the dataset:\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data['data'], columns=['sepal_len', 'sepal_width', 'petal_lengh', 'petal_width'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build your model here\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data['data'], columns=['sepal_len', 'sepal_width', 'petal_lengh', 'petal_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['sepal_width', 'sepal_len']]\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "model = logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_model.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(model, 'logistic_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpickled_model = joblib.load('logistic_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 1.43527432e-19, 7.32330102e-17]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpickled_model.predict_proba([[34, 22]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpickled_model.predict([[34, 22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Serialization\n",
    "\n",
    "After you've fit your `model` object, you can use joblib to save your model from this notebook, which will create a pickle file.  Then copy your `.pkl` file to your Flask service directory, then load it from there to finish your integration.  Check the suggested reading above for further details.\n",
    "\n",
    "**After loading your file into Flask**\n",
    "\n",
    "Once you load your model back into Flask, it will behave just like any other sklearn model object. It will have a `.predict()` method that you can use to finish your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(model, 'my_model.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
