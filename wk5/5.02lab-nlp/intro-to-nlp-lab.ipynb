{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Natural Language Processing Lab\n",
    "\n",
    "_Authors: Dave Yerrington (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "In this lab, we'll explore scikit-learn and NLTK's capabilities for processing text even further. We'll use the 20 newsgroups data set, which is provided by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data science imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the scikit-learn data set:\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use the `fetch_20newsgroups` function to download a training and testing set.\n",
    "\n",
    "The \"20 Newsgroups\" dataset is described [here](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n",
    "\n",
    "For this lab let's choose 4 categories to analyze.  The full list is given below.\n",
    "\n",
    "\n",
    "```python\n",
    "['alt.atheism',\n",
    " 'comp.graphics',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "```\n",
    "\n",
    "Note that the solution code will use these categories:\n",
    "- `alt.atheism`\n",
    "- `talk.religion.misc`\n",
    "- `comp.graphics`\n",
    "- `sci.space`\n",
    "\n",
    "Also remove the headers, footers, and quotes using the `remove` keyword argument of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "#Extracting Information from the Data's Dictionary format \n",
    "\n",
    "categories = ['sci.crypt',\n",
    "              'sci.electronics',\n",
    "              'talk.politics.guns',\n",
    "              'talk.politics.mideast']  # Fill in whatever categories you want to use!!\n",
    "\n",
    "# Setting out training data\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "# Setting our testing data\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What does the `shuffle` argument do?  Why are we setting a `random_state`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A: Shuffle independently and identically distributes the data, random state\n",
    "# allows for reproducability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Inspect the data.\n",
    "\n",
    "We've downloaded a few `newsgroups` categories and removed their headers, footers, and quotes.\n",
    "\n",
    "Because this is a scikit-learn data set, it comes with pre-split training and testing sets (note: we were able to call \"train\" and \"test\" in subset).\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1) What data type is `data_train`?\n",
    "- Is it a list? A dictionary? What else?\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point. What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "type(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Actually, I'm still trying to understand the self-justifying rationale\\nbehind the recent murder of Ian Feinberg (?) in Gaza.\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['target'][0:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Create a bag-of-words model.\n",
    "\n",
    "Let's train a model using a simple count vectorizer.\n",
    "\n",
    "1) Initialize a standard CountVectorizer and fit the training data.\n",
    "- How big is the feature dictionary?\n",
    "- Eliminate English stop words.\n",
    "- Is the dictionary smaller?\n",
    "- Transform the training data using the trained vectorizer.\n",
    "- Evaluate the performance of a logistic regression on the features extracted by the CountVectorizer.\n",
    "    - You will have to transform the `test_set`, too. Be careful to use the trained vectorizer without refitting it.\n",
    "\n",
    "**Bonus**\n",
    "- Try a couple of modifications:\n",
    "    - Restrict the `max_features`.\n",
    "    - Change the `max_df` and `min_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '00000000',\n",
       " '00000000b',\n",
       " '00000001',\n",
       " '00000001b',\n",
       " '00000010',\n",
       " '00000010b',\n",
       " '00000011',\n",
       " '00000011b',\n",
       " '00000100',\n",
       " '00000100b',\n",
       " '00000101',\n",
       " '00000101b',\n",
       " '00000110',\n",
       " '00000110b',\n",
       " '00000111',\n",
       " '00000111b',\n",
       " '00001000',\n",
       " '00001000b',\n",
       " '00001001',\n",
       " '00001001b',\n",
       " '00001010',\n",
       " '00001010b',\n",
       " '00001011',\n",
       " '00001011b',\n",
       " '00001100',\n",
       " '00001100b',\n",
       " '00001101',\n",
       " '00001101b',\n",
       " '00001110',\n",
       " '00001110b',\n",
       " '00001111',\n",
       " '00001111b',\n",
       " '00010000',\n",
       " '00010000b',\n",
       " '00010001',\n",
       " '00010001b',\n",
       " '00010010',\n",
       " '00010010b',\n",
       " '00010011',\n",
       " '00010011b',\n",
       " '00010100',\n",
       " '00010100b',\n",
       " '00010101',\n",
       " '00010101b',\n",
       " '00010110',\n",
       " '00010110b',\n",
       " '00010111',\n",
       " '00010111b',\n",
       " '00011000',\n",
       " '00011000b',\n",
       " '00011001',\n",
       " '00011001b',\n",
       " '00011010',\n",
       " '00011010b',\n",
       " '00011011',\n",
       " '00011011b',\n",
       " '00011100',\n",
       " '00011100b',\n",
       " '00011101',\n",
       " '00011101b',\n",
       " '00011110',\n",
       " '00011110b',\n",
       " '00011111',\n",
       " '00011111b',\n",
       " '000152',\n",
       " '0005895485',\n",
       " '000th',\n",
       " '001',\n",
       " '00100000',\n",
       " '00100000b',\n",
       " '00100001',\n",
       " '00100001b',\n",
       " '00100010',\n",
       " '00100010b',\n",
       " '00100011',\n",
       " '00100011b',\n",
       " '00100100',\n",
       " '00100100b',\n",
       " '00100101',\n",
       " '00100101b',\n",
       " '00100110',\n",
       " '00100110b',\n",
       " '00100111',\n",
       " '00100111b',\n",
       " '00101000',\n",
       " '00101000b',\n",
       " '00101001',\n",
       " '00101001b',\n",
       " '00101010',\n",
       " '00101010b',\n",
       " '00101011',\n",
       " '00101011b',\n",
       " '00101100',\n",
       " '00101100b',\n",
       " '00101101',\n",
       " '00101101b',\n",
       " '00101110',\n",
       " '00101110b',\n",
       " '00101111',\n",
       " '00101111b',\n",
       " '0011',\n",
       " '00110000',\n",
       " '00110000b',\n",
       " '00110001',\n",
       " '00110001b',\n",
       " '00110010',\n",
       " '00110010b',\n",
       " '00110011',\n",
       " '00110011b',\n",
       " '00110100',\n",
       " '00110100b',\n",
       " '00110101',\n",
       " '00110101b',\n",
       " '00110110',\n",
       " '00110110b',\n",
       " '00110111',\n",
       " '00110111b',\n",
       " '00111000',\n",
       " '00111000b',\n",
       " '00111001',\n",
       " '00111001b',\n",
       " '00111010',\n",
       " '00111010b',\n",
       " '00111011',\n",
       " '00111011b',\n",
       " '00111100',\n",
       " '00111100b',\n",
       " '00111101',\n",
       " '00111101b',\n",
       " '00111110',\n",
       " '00111110b',\n",
       " '00111111',\n",
       " '00111111b',\n",
       " '001319',\n",
       " '002',\n",
       " '0024',\n",
       " '003',\n",
       " '0030',\n",
       " '004',\n",
       " '004418',\n",
       " '005',\n",
       " '0058',\n",
       " '007',\n",
       " '0078',\n",
       " '008',\n",
       " '009',\n",
       " '0096b294',\n",
       " '00acearl',\n",
       " '00xkv',\n",
       " '01',\n",
       " '010',\n",
       " '01000000',\n",
       " '01000000b',\n",
       " '01000001',\n",
       " '01000001b',\n",
       " '01000010',\n",
       " '01000010b',\n",
       " '01000011',\n",
       " '01000011b',\n",
       " '01000100',\n",
       " '01000100b',\n",
       " '01000101',\n",
       " '01000101b',\n",
       " '01000110',\n",
       " '01000110b',\n",
       " '01000111',\n",
       " '01000111b',\n",
       " '01001000',\n",
       " '01001000b',\n",
       " '01001001',\n",
       " '01001001b',\n",
       " '01001010',\n",
       " '01001010b',\n",
       " '01001011',\n",
       " '01001011b',\n",
       " '01001100',\n",
       " '01001100b',\n",
       " '01001101',\n",
       " '01001101b',\n",
       " '01001110',\n",
       " '01001110b',\n",
       " '01001111',\n",
       " '01001111b',\n",
       " '01010000',\n",
       " '01010000b',\n",
       " '01010001',\n",
       " '01010001b',\n",
       " '01010010',\n",
       " '01010010b',\n",
       " '01010011',\n",
       " '01010011b',\n",
       " '01010100',\n",
       " '01010100b',\n",
       " '01010101',\n",
       " '01010101b',\n",
       " '01010110',\n",
       " '01010110b',\n",
       " '01010111',\n",
       " '01010111b',\n",
       " '01011000',\n",
       " '01011000b',\n",
       " '01011001',\n",
       " '01011001b',\n",
       " '01011010',\n",
       " '01011010b',\n",
       " '01011011',\n",
       " '01011011b',\n",
       " '01011100',\n",
       " '01011100b',\n",
       " '01011101',\n",
       " '01011101b',\n",
       " '01011110',\n",
       " '01011110b',\n",
       " '01011111',\n",
       " '01011111b',\n",
       " '011',\n",
       " '01100000',\n",
       " '01100000b',\n",
       " '01100001',\n",
       " '01100001b',\n",
       " '01100010',\n",
       " '01100010b',\n",
       " '01100011',\n",
       " '01100011b',\n",
       " '01100100',\n",
       " '01100100b',\n",
       " '01100101',\n",
       " '01100101b',\n",
       " '01100110',\n",
       " '01100110b',\n",
       " '01100111',\n",
       " '01100111b',\n",
       " '01101000',\n",
       " '01101000b',\n",
       " '01101001',\n",
       " '01101001b',\n",
       " '01101010',\n",
       " '01101010b',\n",
       " '01101011',\n",
       " '01101011b',\n",
       " '01101100',\n",
       " '01101100b',\n",
       " '01101101',\n",
       " '01101101b',\n",
       " '01101110',\n",
       " '01101110b',\n",
       " '01101111',\n",
       " '01101111b',\n",
       " '01110000',\n",
       " '01110000b',\n",
       " '01110001',\n",
       " '01110001b',\n",
       " '01110010',\n",
       " '01110010b',\n",
       " '01110011',\n",
       " '01110011b',\n",
       " '01110100',\n",
       " '01110100b',\n",
       " '01110101',\n",
       " '01110101b',\n",
       " '01110110',\n",
       " '01110110b',\n",
       " '01110111',\n",
       " '01110111b',\n",
       " '01111000',\n",
       " '01111000b',\n",
       " '01111001',\n",
       " '01111001b',\n",
       " '01111010',\n",
       " '01111010b',\n",
       " '01111011',\n",
       " '01111011b',\n",
       " '01111100',\n",
       " '01111100b',\n",
       " '01111101',\n",
       " '01111101b',\n",
       " '01111110',\n",
       " '01111110b',\n",
       " '01111111',\n",
       " '01111111b',\n",
       " '0115',\n",
       " '012',\n",
       " '012011',\n",
       " '013',\n",
       " '014',\n",
       " '014t4',\n",
       " '015',\n",
       " '016',\n",
       " '01720',\n",
       " '01730',\n",
       " '01760',\n",
       " '018',\n",
       " '0188',\n",
       " '01880',\n",
       " '01h5',\n",
       " '02',\n",
       " '02026',\n",
       " '02139',\n",
       " '02174',\n",
       " '022',\n",
       " '02215',\n",
       " '0235',\n",
       " '025258',\n",
       " '02790',\n",
       " '02903',\n",
       " '03',\n",
       " '0300',\n",
       " '03083',\n",
       " '031349',\n",
       " '032828',\n",
       " '033',\n",
       " '033213',\n",
       " '0335',\n",
       " '034',\n",
       " '035',\n",
       " '037',\n",
       " '04',\n",
       " '0400',\n",
       " '042427',\n",
       " '045',\n",
       " '0453',\n",
       " '047',\n",
       " '04e',\n",
       " '04glvr8a',\n",
       " '05',\n",
       " '0500',\n",
       " '053',\n",
       " '0530',\n",
       " '0531',\n",
       " '055',\n",
       " '0565',\n",
       " '059',\n",
       " '05qnf',\n",
       " '05rov',\n",
       " '06',\n",
       " '0600h',\n",
       " '06066',\n",
       " '06108',\n",
       " '0678',\n",
       " '07',\n",
       " '07059',\n",
       " '07090',\n",
       " '0714',\n",
       " '0715',\n",
       " '073',\n",
       " '074',\n",
       " '077',\n",
       " '0785',\n",
       " '08',\n",
       " '0837',\n",
       " '08502',\n",
       " '08h',\n",
       " '09',\n",
       " '092',\n",
       " '092101',\n",
       " '0952',\n",
       " '0962',\n",
       " '0_',\n",
       " '0_35r',\n",
       " '0_e8',\n",
       " '0a',\n",
       " '0as',\n",
       " '0b',\n",
       " '0b1fatransfer',\n",
       " '0b800h',\n",
       " '0b9w',\n",
       " '0bs',\n",
       " '0bsjom',\n",
       " '0c',\n",
       " '0c_',\n",
       " '0c_l',\n",
       " '0ct1t',\n",
       " '0d',\n",
       " '0e',\n",
       " '0e0',\n",
       " '0eg6g',\n",
       " '0er',\n",
       " '0f',\n",
       " '0ga',\n",
       " '0gn',\n",
       " '0gx',\n",
       " '0h',\n",
       " '0h6o481w8h1t2',\n",
       " '0hyx',\n",
       " '0i',\n",
       " '0i1dg',\n",
       " '0i91n',\n",
       " '0im',\n",
       " '0j',\n",
       " '0ja',\n",
       " '0jd',\n",
       " '0jp',\n",
       " '0k',\n",
       " '0k5',\n",
       " '0ki',\n",
       " '0ks',\n",
       " '0l_',\n",
       " '0lq',\n",
       " '0m',\n",
       " '0m6',\n",
       " '0ma',\n",
       " '0mis',\n",
       " '0mjx9',\n",
       " '0n',\n",
       " '0nds8',\n",
       " '0niy',\n",
       " '0np34',\n",
       " '0nt',\n",
       " '0nu',\n",
       " '0o',\n",
       " '0p',\n",
       " '0p7',\n",
       " '0pn',\n",
       " '0pz',\n",
       " '0q',\n",
       " '0r',\n",
       " '0rdf',\n",
       " '0rljas',\n",
       " '0s7l',\n",
       " '0s8',\n",
       " '0sw',\n",
       " '0sy3',\n",
       " '0t0tl_b',\n",
       " '0tgm7',\n",
       " '0th',\n",
       " '0u',\n",
       " '0v',\n",
       " '0v7fs',\n",
       " '0v9g',\n",
       " '0vfhtw8s',\n",
       " '0vuzc',\n",
       " '0vx48',\n",
       " '0wa',\n",
       " '0wc',\n",
       " '0wh',\n",
       " '0x',\n",
       " '0x01',\n",
       " '0xj',\n",
       " '0xs',\n",
       " '0y',\n",
       " '0y0',\n",
       " '0ycf',\n",
       " '0z',\n",
       " '0z6m',\n",
       " '0zbozv',\n",
       " '0zu',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '100000',\n",
       " '10000000',\n",
       " '10000000b',\n",
       " '10000001',\n",
       " '10000001b',\n",
       " '10000010',\n",
       " '10000010b',\n",
       " '10000011',\n",
       " '10000011b',\n",
       " '10000100',\n",
       " '10000100b',\n",
       " '10000101',\n",
       " '10000101b',\n",
       " '10000110',\n",
       " '10000110b',\n",
       " '10000111',\n",
       " '10000111b',\n",
       " '10001000',\n",
       " '10001000b',\n",
       " '10001001',\n",
       " '10001001b',\n",
       " '10001010',\n",
       " '10001010b',\n",
       " '10001011',\n",
       " '10001011b',\n",
       " '10001100',\n",
       " '10001100b',\n",
       " '10001101',\n",
       " '10001101b',\n",
       " '10001110',\n",
       " '10001110b',\n",
       " '10001111',\n",
       " '10001111b',\n",
       " '100014',\n",
       " '10010000',\n",
       " '10010000b',\n",
       " '10010001',\n",
       " '10010001b',\n",
       " '10010010',\n",
       " '100100101111',\n",
       " '10010010b',\n",
       " '10010011',\n",
       " '10010011b',\n",
       " '10010100',\n",
       " '10010100b',\n",
       " '10010101',\n",
       " '10010101b',\n",
       " '10010110',\n",
       " '10010110b',\n",
       " '10010111',\n",
       " '10010111b',\n",
       " '10011000',\n",
       " '10011000b',\n",
       " '10011001',\n",
       " '10011001b',\n",
       " '10011010',\n",
       " '10011010b',\n",
       " '10011011',\n",
       " '10011011b',\n",
       " '10011100',\n",
       " '10011100b',\n",
       " '10011101',\n",
       " '10011101b',\n",
       " '10011110',\n",
       " '10011110b',\n",
       " '10011111',\n",
       " '10011111b',\n",
       " '10018',\n",
       " '100bhp',\n",
       " '100db',\n",
       " '100h',\n",
       " '100k',\n",
       " '100ma',\n",
       " '100mfd',\n",
       " '100mhz',\n",
       " '100uf',\n",
       " '100x',\n",
       " '101',\n",
       " '10100000',\n",
       " '10100000b',\n",
       " '10100001',\n",
       " '10100001b',\n",
       " '10100010',\n",
       " '10100010b',\n",
       " '10100011',\n",
       " '10100011b',\n",
       " '10100100',\n",
       " '10100100b',\n",
       " '10100101',\n",
       " '10100101b',\n",
       " '10100110',\n",
       " '10100110b',\n",
       " '10100111',\n",
       " '10100111b',\n",
       " '10101000',\n",
       " '10101000b',\n",
       " '10101001',\n",
       " '10101001b',\n",
       " '10101010',\n",
       " '10101010b',\n",
       " '10101011',\n",
       " '10101011b',\n",
       " '10101100',\n",
       " '10101100b',\n",
       " '10101101',\n",
       " '10101101b',\n",
       " '10101110',\n",
       " '10101110b',\n",
       " '10101111',\n",
       " '10101111b',\n",
       " '10110000',\n",
       " '10110000b',\n",
       " '10110001',\n",
       " '10110001b',\n",
       " '10110010',\n",
       " '10110010b',\n",
       " '10110011',\n",
       " '10110011b',\n",
       " '10110100',\n",
       " '10110100b',\n",
       " '10110101',\n",
       " '10110101b',\n",
       " '10110110',\n",
       " '10110110b',\n",
       " '10110111',\n",
       " '10110111b',\n",
       " '10111000',\n",
       " '10111000b',\n",
       " '10111001',\n",
       " '10111001b',\n",
       " '10111010',\n",
       " '10111010b',\n",
       " '10111011',\n",
       " '10111011b',\n",
       " '10111100',\n",
       " '10111100b',\n",
       " '10111101',\n",
       " '10111101b',\n",
       " '10111110',\n",
       " '10111110b',\n",
       " '10111111',\n",
       " '10111111b',\n",
       " '101356',\n",
       " '1016',\n",
       " '101st',\n",
       " '102',\n",
       " '1020',\n",
       " '1024',\n",
       " '10248b',\n",
       " '1025',\n",
       " '1026',\n",
       " '102nd',\n",
       " '103',\n",
       " '1031',\n",
       " '1032',\n",
       " '1033',\n",
       " '1036',\n",
       " '10367',\n",
       " '103895',\n",
       " '103d',\n",
       " '103rd',\n",
       " '103th',\n",
       " '104',\n",
       " '1042',\n",
       " '1045',\n",
       " '105',\n",
       " '1053',\n",
       " '1054',\n",
       " '1055',\n",
       " '1056',\n",
       " '106',\n",
       " '107',\n",
       " '1070',\n",
       " '108',\n",
       " '1080',\n",
       " '10886',\n",
       " '109',\n",
       " '1090',\n",
       " '1091',\n",
       " '10930',\n",
       " '1095',\n",
       " '1097',\n",
       " '1098',\n",
       " '109th',\n",
       " '10c',\n",
       " '10dbv',\n",
       " '10h',\n",
       " '10k',\n",
       " '10khz',\n",
       " '10m',\n",
       " '10mhz',\n",
       " '10s',\n",
       " '10th',\n",
       " '10vdc',\n",
       " '10w',\n",
       " '10x',\n",
       " '10xlog',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11000000',\n",
       " '11000000b',\n",
       " '11000001',\n",
       " '11000001b',\n",
       " '11000010',\n",
       " '11000010b',\n",
       " '11000011',\n",
       " '11000011b',\n",
       " '11000100',\n",
       " '11000100b',\n",
       " '11000101',\n",
       " '11000101b',\n",
       " '11000110',\n",
       " '11000110b',\n",
       " '11000111',\n",
       " '11000111b',\n",
       " '11001000',\n",
       " '11001000b',\n",
       " '11001001',\n",
       " '11001001b',\n",
       " '11001010',\n",
       " '11001010b',\n",
       " '11001011',\n",
       " '11001011b',\n",
       " '11001100',\n",
       " '11001100b',\n",
       " '11001101',\n",
       " '11001101b',\n",
       " '11001110',\n",
       " '11001110b',\n",
       " '11001111',\n",
       " '11001111b',\n",
       " '11010000',\n",
       " '11010000b',\n",
       " '11010001',\n",
       " '11010001b',\n",
       " '11010010',\n",
       " '11010010b',\n",
       " '11010011',\n",
       " '11010011b',\n",
       " '11010100',\n",
       " '11010100b',\n",
       " '11010101',\n",
       " '11010101b',\n",
       " '11010110',\n",
       " '11010110b',\n",
       " '11010111',\n",
       " '11010111b',\n",
       " '11011000',\n",
       " '11011000b',\n",
       " '11011001',\n",
       " '11011001b',\n",
       " '11011010',\n",
       " '11011010b',\n",
       " '11011011',\n",
       " '11011011b',\n",
       " '11011100',\n",
       " '11011100b',\n",
       " '11011101',\n",
       " '11011101b',\n",
       " '11011110',\n",
       " '11011110b',\n",
       " '11011111',\n",
       " '11011111b',\n",
       " '1107',\n",
       " '110hz',\n",
       " '110v',\n",
       " '110vac',\n",
       " '111',\n",
       " '11100000',\n",
       " '11100000b',\n",
       " '11100001',\n",
       " '11100001b',\n",
       " '11100010',\n",
       " '11100010b',\n",
       " '11100011',\n",
       " '11100011b',\n",
       " '11100100',\n",
       " '11100100b',\n",
       " '11100101',\n",
       " '11100101b',\n",
       " '11100110',\n",
       " '11100110b',\n",
       " '11100111',\n",
       " '11100111b',\n",
       " '11101000',\n",
       " '11101000b',\n",
       " '11101001',\n",
       " '11101001b',\n",
       " '11101010',\n",
       " '11101010b',\n",
       " '11101011',\n",
       " '11101011b',\n",
       " '11101100',\n",
       " '11101100b',\n",
       " '11101101',\n",
       " '11101101b',\n",
       " '11101110',\n",
       " '11101110b',\n",
       " '11101111',\n",
       " '11101111b',\n",
       " '11110000',\n",
       " '11110000b',\n",
       " '11110001',\n",
       " '11110001b',\n",
       " '11110010',\n",
       " '11110010b',\n",
       " '11110011',\n",
       " '11110011b',\n",
       " '11110100',\n",
       " '11110100b',\n",
       " '11110101',\n",
       " '11110101b',\n",
       " '11110110',\n",
       " '11110110b',\n",
       " '11110111',\n",
       " '11110111b',\n",
       " '11111000',\n",
       " '11111000b',\n",
       " '11111001',\n",
       " '11111001b',\n",
       " '11111010',\n",
       " '11111010b',\n",
       " '11111011',\n",
       " '11111011b',\n",
       " '11111100',\n",
       " '11111100b',\n",
       " '11111101',\n",
       " '11111101b',\n",
       " '11111110',\n",
       " '11111110b',\n",
       " '11111111',\n",
       " '11111111b',\n",
       " '1113',\n",
       " '1114',\n",
       " '1115',\n",
       " '1118',\n",
       " '1119',\n",
       " '112',\n",
       " '1126',\n",
       " '1128',\n",
       " '1129',\n",
       " '113',\n",
       " '11300',\n",
       " '1133',\n",
       " '1135',\n",
       " '114',\n",
       " '1140',\n",
       " '1149',\n",
       " '115',\n",
       " '11544',\n",
       " '11548',\n",
       " '116',\n",
       " '11613',\n",
       " '117',\n",
       " '1174',\n",
       " '117441',\n",
       " '1175',\n",
       " '1177',\n",
       " '117v',\n",
       " '118',\n",
       " '118jheoyo',\n",
       " '11a',\n",
       " '11b',\n",
       " '11pm',\n",
       " '11qm9',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12002',\n",
       " '12003',\n",
       " '1201',\n",
       " '12012',\n",
       " '1206',\n",
       " '1207',\n",
       " '1208',\n",
       " '120rtn_____________________',\n",
       " '120th',\n",
       " '120v',\n",
       " '120vac',\n",
       " '121',\n",
       " '1212',\n",
       " '1213',\n",
       " '1214',\n",
       " '1217',\n",
       " '122',\n",
       " '122057',\n",
       " '1229',\n",
       " '123',\n",
       " '1233',\n",
       " '124',\n",
       " '1240',\n",
       " '1242',\n",
       " '125',\n",
       " '1256',\n",
       " '126',\n",
       " '1262',\n",
       " '127',\n",
       " '127223',\n",
       " '1276',\n",
       " '128',\n",
       " '1283',\n",
       " '1285',\n",
       " '128k',\n",
       " '129',\n",
       " '1290',\n",
       " '1296',\n",
       " '12ga',\n",
       " '12pt',\n",
       " '12t',\n",
       " '12th',\n",
       " '12v',\n",
       " '12vac',\n",
       " '12vdc',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '130c',\n",
       " '130o8kp3j',\n",
       " '130xe',\n",
       " '131',\n",
       " '1310',\n",
       " '1316',\n",
       " '132',\n",
       " '1320',\n",
       " '1321',\n",
       " '133',\n",
       " '1331y',\n",
       " '1333',\n",
       " '134',\n",
       " '13401',\n",
       " '1341',\n",
       " '135',\n",
       " '1350',\n",
       " '1354',\n",
       " '1355',\n",
       " '136',\n",
       " '137',\n",
       " '1371',\n",
       " '1373',\n",
       " '137g',\n",
       " '138',\n",
       " '1385',\n",
       " '139',\n",
       " '13hzj',\n",
       " '13kbit',\n",
       " '13m',\n",
       " '13s0',\n",
       " '13th',\n",
       " '13y',\n",
       " '13zy',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1404',\n",
       " '141',\n",
       " '1412',\n",
       " '1417',\n",
       " '1421',\n",
       " '1422',\n",
       " '1423',\n",
       " '1424',\n",
       " '14262',\n",
       " '143',\n",
       " '1430',\n",
       " '1435',\n",
       " '144',\n",
       " '1440',\n",
       " '1447',\n",
       " '145',\n",
       " '1453',\n",
       " '14571',\n",
       " '146',\n",
       " '1463',\n",
       " '147',\n",
       " '14766',\n",
       " '148',\n",
       " '1484',\n",
       " '1488',\n",
       " '1489',\n",
       " '149',\n",
       " '1496a',\n",
       " '14e6',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15010',\n",
       " '150mhz',\n",
       " '150ohm',\n",
       " '150ua',\n",
       " '151',\n",
       " '1516',\n",
       " '152',\n",
       " '152140',\n",
       " '15219',\n",
       " '1524',\n",
       " '152937',\n",
       " '153',\n",
       " '1530',\n",
       " '15301',\n",
       " '1531',\n",
       " '1536',\n",
       " '154',\n",
       " '1541b',\n",
       " '1545',\n",
       " '155',\n",
       " '1550',\n",
       " '1554',\n",
       " '1559',\n",
       " '155mm',\n",
       " '156',\n",
       " '157',\n",
       " '1570',\n",
       " '1571',\n",
       " '1578',\n",
       " '158',\n",
       " '158288',\n",
       " '159',\n",
       " '15a',\n",
       " '15amp',\n",
       " '15i6x',\n",
       " '15k',\n",
       " '15mv',\n",
       " '15th',\n",
       " '15xne',\n",
       " '15yjt',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '160mph',\n",
       " '161',\n",
       " '1615',\n",
       " '1615a',\n",
       " '162',\n",
       " '1620',\n",
       " '163',\n",
       " '16384',\n",
       " '164',\n",
       " '1647',\n",
       " '165',\n",
       " '1650',\n",
       " '166',\n",
       " '1673',\n",
       " '168',\n",
       " '1684',\n",
       " '1686',\n",
       " '169',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of hot garbage, at least at the top\n",
    "cvec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33374"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This might explain it\n",
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(data_train['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33069"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(cvec.transform(data_train['data']).todense(), columns=cvec.get_feature_names())\n",
    "y_train = data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "people        1605\n",
       "don           1031\n",
       "like           989\n",
       "use            955\n",
       "key            943\n",
       "just           937\n",
       "know           888\n",
       "government     815\n",
       "time           745\n",
       "said           677\n",
       "think          658\n",
       "right          657\n",
       "gun            634\n",
       "armenian       611\n",
       "used           593\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most common terms\n",
    "X_train.sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1529"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(cvec.transform(data_test['data']).todense(), columns=cvec.get_feature_names())\n",
    "y_test = data_test['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7985611510791367"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Test Out Hashing and TF-IDF.\n",
    "\n",
    "Let's see if hashing or TF-IDF improves our accuracy.\n",
    "\n",
    "1) Initialize a HashingVectorizer and repeat the test with no restriction on the number of features.\n",
    "- Does the score improve with respect to the CountVectorizer?\n",
    "- Print out the number of features for this model.\n",
    "- Initialize a TF-IDF vectorizer and repeat the analysis above.\n",
    "- Print out the number of features for this model.\n",
    "\n",
    "**Bonus**\n",
    "- Change the parameters of either (or both) models to improve your score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A:\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hvec = HashingVectorizer(stop_words='english')\n",
    "#lr = LogisticRegression()\n",
    "\n",
    "#X_train = data_train['data']\n",
    "#hvec.fit(X_train, y_train)\n",
    "#X_test = data_test['data']\n",
    "\n",
    "#(X_train, y_train)\n",
    "#preds = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfw you realize that you have no idea how to fit nlp vectorizers outside of pipes\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "X_train = data_train['data']\n",
    "X_test = data_test['data']\n",
    "\n",
    "model = make_pipeline(HashingVectorizer(stop_words='english'), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1529"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1529"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8024852844996729"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8260300850228908"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = data_train['data']\n",
    "X_test = data_test['data']\n",
    "\n",
    "model = make_pipeline(TfidfVectorizer(stop_words='english'), LogisticRegression())\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2296"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of features if I'm not mistaken\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. [Bonus] Robust Text Preprocessing\n",
    "\n",
    "Your mission, should you choose to accept it, is to write a preprocessing function for all of your text.  This functions should\n",
    "\n",
    "- convert all text to lowercase,\n",
    "- remove punctuation,\n",
    "- stem or lemmatize each word of the text,\n",
    "- remove stopwords.\n",
    "\n",
    "The function should receive one string of text and return the processed text.\n",
    "\n",
    "Once you have built your function, use it to process your train and test data, then fit a Logistic Regression model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
