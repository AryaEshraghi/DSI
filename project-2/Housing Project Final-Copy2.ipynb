{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: Predict home sale prices\n",
    "\n",
    "#### Approach: Use LASSO to generate a model from a dense and minimally pruned feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Checking relative shapes of datasets. Test set is missing target column as\n",
    "# expected. Parity between the two DataFrames is imperative to making\n",
    "# predictions. As such, until I find a better way, parity will be manually\n",
    "# maintained\n",
    "print('Training set shape:', df.shape)\n",
    "print('Test set shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # Plenty of non-numericl data and nulls to clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Manually determine which categorical features have promise as dummies.\n",
    "# Criteria are as follows:\n",
    "# 1) Not too homogenous - if there is one dominent category, it won't be\n",
    "# significant\n",
    "# 2) Not too sparse. The data needs to speak to us. To do that, it must be there\n",
    "# 3) Not too scattered. Too many values generates more columns than it may be\n",
    "# worth working with\n",
    "# 4) Must be non-ordinal\n",
    "# ***************************************************************************** #\n",
    "# Since we will be using LASSO to cull our feature set, I'm not too worried if\n",
    "# some of these don't add much. They will be dealt with when the time comes.\n",
    "# The commented lines were deemed unfit for inclusion.\n",
    "\n",
    "#df['Street'].value_counts()\n",
    "df['Year Built'].value_counts()\n",
    "#df['Land Contour'].value_counts()\n",
    "#df['Functional'].value_counts()\n",
    "df['Neighborhood'].value_counts()\n",
    "#df['Land Slope'].value_counts()\n",
    "df['MS SubClass'].value_counts()\n",
    "df['MS Zoning'].value_counts()\n",
    "#df['Alley'].value_counts() # Mostly null\n",
    "df['Lot Shape'].value_counts()\n",
    "#df['Utilities'].value_counts()\n",
    "df['Lot Config'].value_counts()\n",
    "df['Condition 1'].value_counts()\n",
    "#df['Condition 2'].value_counts()a\n",
    "df['Bldg Type'].value_counts()\n",
    "df['House Style'].value_counts()\n",
    "df['Year Built'].value_counts() # Non-ordinal\n",
    "df['Year Remod/Add'].value_counts() # Non-ordinal\n",
    "#df['Roof Style'].value_counts()\n",
    "#df['Roof Matl'].value_counts()\n",
    "df['Exterior 1st'].value_counts()\n",
    "#df['Exterior 2nd'].value_counts()\n",
    "df['Mas Vnr Type'].value_counts()\n",
    "df['Exter Qual'].value_counts()\n",
    "df['Exter Cond'].value_counts()\n",
    "df['Foundation'].value_counts()\n",
    "df['Bsmt Qual'].value_counts()\n",
    "#df['Bsmt Cond'].value_counts()\n",
    "df['Bsmt Exposure'].value_counts()\n",
    "df['BsmtFin Type 1'].value_counts()\n",
    "#df['BsmtFin Type 2'].value_counts() # Not many with 2 basements\n",
    "#df['Heating'].value_counts() # Too homogenous\n",
    "df['Heating QC'].value_counts()\n",
    "df['Central Air'].value_counts() # Maybe... Only one column...\n",
    "#df['Electrical'].value_counts() # Too homogenous\n",
    "df['Kitchen Qual'].value_counts()\n",
    "#df['Functional'].value_counts() # Too homogenous\n",
    "#df['Fireplace Qu'].value_counts() # Mostly null\n",
    "df['Garage Type'].value_counts()\n",
    "#df['Garage Yr Blt'].value_counts() # Non ordinal, though newer is better I guess\n",
    "df['Garage Finish'].value_counts()\n",
    "#df['Garage Qual'].value_counts() # Too homogenous\n",
    "#df['Garage Cond'].value_counts() # Too homogenous\n",
    "df['Paved Drive'].value_counts() # Pretty homogenous but also only adds two columns...\n",
    "#df['Pool QC'].value_counts() # Mostly null values\n",
    "#df['Fence'].value_counts() # Mostly null values\n",
    "##df['Misc Feature'].value_counts() # Mostly null but always adds value to property\n",
    "##df['Misc Val'].value_counts()\n",
    "df['Mo Sold'].value_counts() # Non ordinal. Is it even relevant?\n",
    "df['Yr Sold'].value_counts() # Non ordinal; prices do change through the years\n",
    "df['Sale Type'].value_counts() # No idea what it means but it's worth checking out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns composed largely of null values. Still on the fence about\n",
    "# the Misc Features category\n",
    "df = df.drop(columns=['Alley', 'Pool QC', 'Fence', 'Misc Feature',\n",
    "                      'Lot Frontage', 'Fireplace Qu'])\n",
    "test = test.drop(columns=['Alley', 'Pool QC', 'Fence', 'Misc Feature',\n",
    "                          'Lot Frontage', 'Fireplace Qu'])\n",
    "# Drop non-numeric columns that are not good dummy candidates\n",
    "df.drop(columns=['Street', 'Land Contour', 'Functional', 'Land Slope',\n",
    "                     'Utilities', 'Condition 2', 'Roof Style', 'Roof Matl',\n",
    "                     'Exterior 2nd', 'Bsmt Cond', 'BsmtFin Type 2', 'Heating',\n",
    "                     'Electrical', 'Functional', 'Garage Yr Blt', 'Garage Qual',\n",
    "                     'Garage Cond'], inplace=True)\n",
    "test.drop(columns=['Street', 'Land Contour', 'Functional', 'Land Slope',\n",
    "                     'Utilities', 'Condition 2', 'Roof Style', 'Roof Matl',\n",
    "                     'Exterior 2nd', 'Bsmt Cond', 'BsmtFin Type 2', 'Heating',\n",
    "                     'Electrical', 'Functional', 'Garage Yr Blt', 'Garage Qual',\n",
    "                     'Garage Cond'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations between features and target using a heatmap\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining columns with low correlation to target before scaling, since\n",
    "# correlation is independent of scale and origin\n",
    "df.drop(columns=['Pool Area', 'Screen Porch', '3Ssn Porch', 'Enclosed Porch',\n",
    "                'Kitchen AbvGr', 'Bedroom AbvGr', 'Bsmt Half Bath',\n",
    "                'Low Qual Fin SF', 'BsmtFin SF 2', 'PID'], inplace=True)\n",
    "test.drop(columns=['Pool Area', 'Screen Porch', '3Ssn Porch', 'Enclosed Porch',\n",
    "                'Kitchen AbvGr', 'Bedroom AbvGr', 'Bsmt Half Bath',\n",
    "                'Low Qual Fin SF', 'BsmtFin SF 2', 'PID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummies with potential value\n",
    "df = pd.get_dummies(df, columns=['Year Built', 'Neighborhood', 'MS SubClass',\n",
    "               'MS Zoning', 'Lot Shape', 'Lot Config', 'Condition 1',\n",
    "               'Bldg Type', 'House Style', 'Year Remod/Add', 'Exterior 1st',\n",
    "               'Mas Vnr Type', 'Foundation', 'Garage Type',\n",
    "               'Paved Drive', 'Mo Sold', 'Yr Sold', 'Sale Type'],\n",
    "                    drop_first=True)\n",
    "test = pd.get_dummies(test, columns=['Year Built', 'Neighborhood', 'MS SubClass',\n",
    "               'MS Zoning', 'Lot Shape', 'Lot Config', 'Condition 1',\n",
    "               'Bldg Type', 'House Style', 'Year Remod/Add', 'Exterior 1st',\n",
    "               'Mas Vnr Type', 'Foundation', 'Garage Type',\n",
    "               'Paved Drive', 'Mo Sold', 'Yr Sold', 'Sale Type'],\n",
    "                      drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info(), '\\n')\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exterior 1st_ImStucc', 'MS SubClass_150', 'Year Built_1879', 'Year Built_1895', 'MS Zoning_C (all)', 'Neighborhood_GrnHill', 'Year Built_1929', 'Year Built_1898', 'Exterior 1st_CBlock', 'Neighborhood_Landmrk', 'Year Built_1942', 'Year Built_1901', 'Exterior 1st_Stone', 'Year Built_1893', 'Year Built_1896', 'Year Built_1880', 'Year Built_1875', 'Year Built_1911', 'Year Built_1913', 'SalePrice'}\n"
     ]
    }
   ],
   "source": [
    "# Determine what dummy columns the training set has in excess of our test set\n",
    "missing_cols = set(df.columns) - set(test.columns)\n",
    "print(missing_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Year Built_1882', 'Exterior 1st_PreCast', 'Year Built_1904', 'Year Built_1907', 'Year Built_1906', 'Sale Type_VWD', 'Year Built_1902', 'Mas Vnr Type_CBlock'}\n"
     ]
    }
   ],
   "source": [
    "# Determine what additional dummy columns the test set has in relation to the training set\n",
    "add_cols = set(test.columns) - set(df.columns)\n",
    "print(add_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target column before it is dropped\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore parity between datasets\n",
    "df.drop(columns=missing_cols, inplace=True)\n",
    "test.drop(columns=add_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Garage Finish     114\n",
       "Bsmt Exposure      58\n",
       "Bsmt Qual          55\n",
       "BsmtFin Type 1     55\n",
       "Mas Vnr Area       22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find location of null vals\n",
    "df.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all remaining null cells with 0's in order to model\n",
    "df.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries to change ordinal values to be numerically represented\n",
    "bsqual_dict = {'NA': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5} # also use\n",
    "# for ex_qual, ex_cond, ht_qc, kit_qual\n",
    "bsexp_dict = {'NA': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "bsfin_dict = {'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "ac_dict = {'N': 0, 'Y': 1}\n",
    "grfin_dict = {'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change ordinal columns with strings to numeric ordinal values\n",
    "df['Bsmt Qual'].replace(bsqual_dict, inplace=True)\n",
    "df['Bsmt Exposure'].replace(bsexp_dict, inplace=True)\n",
    "df['BsmtFin Type 1'].replace(bsfin_dict, inplace=True)\n",
    "df['Exter Qual'].replace(bsqual_dict, inplace=True)\n",
    "df['Exter Cond'].replace(bsqual_dict, inplace=True)\n",
    "df['Heating QC'].replace(bsqual_dict, inplace=True)\n",
    "df['Kitchen Qual'].replace(bsqual_dict, inplace=True)\n",
    "df['Central Air'].replace(ac_dict, inplace=True)\n",
    "df['Garage Finish'].replace(grfin_dict, inplace=True)\n",
    "\n",
    "test['Bsmt Qual'].replace(bsqual_dict, inplace=True)\n",
    "test['Bsmt Exposure'].replace(bsexp_dict, inplace=True)\n",
    "test['BsmtFin Type 1'].replace(bsfin_dict, inplace=True)\n",
    "test['Exter Qual'].replace(bsqual_dict, inplace=True)\n",
    "test['Exter Cond'].replace(bsqual_dict, inplace=True)\n",
    "test['Heating QC'].replace(bsqual_dict, inplace=True)\n",
    "test['Kitchen Qual'].replace(bsqual_dict, inplace=True)\n",
    "test['Central Air'].replace(ac_dict, inplace=True)\n",
    "test['Garage Finish'].replace(grfin_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate promising interaction terms\n",
    "bsmt_features = df[['BsmtFin Type 1', 'BsmtFin SF 1', 'Bsmt Unf SF', 'Total Bsmt SF',\n",
    "                'Bsmt Full Bath']]\n",
    "gr_features = df[['Garage Cars', 'Garage Area', 'Garage Finish']]\n",
    "int_features = df[['1st Flr SF', '2nd Flr SF', 'Gr Liv Area', 'Full Bath', 'Half Bath',\n",
    "               'Kitchen Qual', 'TotRms AbvGrd', 'Fireplaces']]\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "bsmt_poly = poly.fit_transform(bsmt_features)\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "gr_poly = poly.fit_transform(gr_features)\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "int_poly = poly.fit_transform(int_features)\n",
    "\n",
    "# *************************** Test set ************************************** #\n",
    "bsmt_features = test[['BsmtFin Type 1', 'BsmtFin SF 1', 'Bsmt Unf SF', 'Total Bsmt SF',\n",
    "                'Bsmt Full Bath']]\n",
    "gr_features = test[['Garage Cars', 'Garage Area', 'Garage Finish']]\n",
    "int_features = test[['1st Flr SF', '2nd Flr SF', 'Gr Liv Area', 'Full Bath', 'Half Bath',\n",
    "               'Kitchen Qual', 'TotRms AbvGrd', 'Fireplaces']]\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "bsmt_poly = poly.fit_transform(bsmt_features)\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "gr_poly = poly.fit_transform(gr_features)\n",
    "\n",
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "int_poly = poly.fit_transform(int_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X\n",
    "exclusion = ['SalePrice']\n",
    "features = [col for col in df.columns if not col in exclusion]\n",
    "X = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01218917601170161"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate baseline\n",
    "max(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTS\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Pipe and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a basic linear regression to see how good of a fit we can achieve\n",
    "# without regularization\n",
    "ss = StandardScaler()\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression()\n",
    "lr_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lr', lr),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despite lacking parameters, GridSearch offers integrated CV which makes it\n",
    "# preferable to pipe.fit() and so on.\n",
    "lr_params = {}\n",
    "gs = GridSearchCV(lr_pipe, lr_params, cv=5)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Scores. It doesn't make sense for the gs.score to be as\n",
    "# low as it is. That being said, there is no harm in leaving that to Kaggle to\n",
    "# decide.\n",
    "### Upon rerunning the code, the RMSE has gone to a more reasonable value.\n",
    "print(gs.best_score_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gs.predict(X)\n",
    "print('RMSE for Linear Model w/o Regularization:',\n",
    "      mean_squared_error(y, pred) ** .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Pipe and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Though it's unlikely to perform well at all given the current form of the\n",
    "# data, it doesn't hurt to try and see if kNN offers value.\n",
    "ss = StandardScaler()\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier()\n",
    "knn_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('knn', knn)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "     'knn__n_neighbors': range(7, 25, 2)   \n",
    "}\n",
    "gs = GridSearchCV(knn_pipe, param_grid=knn_params, cv=5)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Scores - not too promising as predicted. Not worth checking RMSE.\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO Pipe and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the massive number of columns and lack of aggressive feature\n",
    "# engineering, my hopes are highest for the LASSO model's preformance\n",
    "ss = StandardScaler()\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lasso', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'lasso__max_iter': [10000], 'lasso__alpha': [3500, 3510, 3520, 3530, 3540, 3550, 3560, 3570, 3580, 3590, 3600, 3610, 3620, 3630, 3640, 3650, 3660, 3670, 3680, 3690, 3700, 3710, 3720, 3730, 3740, 3750, 3760, 3770, 3780, 3790, 3800, 3810, 3820, 3830, 3840, 3850, 3860, 3870, 3880, 3890, 390...30, 4340, 4350, 4360, 4370, 4380, 4390, 4400, 4410, 4420, 4430, 4440, 4450, 4460, 4470, 4480, 4490]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I don't know what exactly to expect, nor what range my alpha values can fall\n",
    "# in, so I'll cast a wide net and see what I catch.\n",
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': [(x * 10) + 3500 for x in range(100)]\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3) # CV = 3 for time\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8113047628600992\n",
      "{'lasso__alpha': 3500, 'lasso__max_iter': 10000}\n",
      "0.8813379590898037\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for LASSO Model (alpha = 525): 31092.381934088353\n"
     ]
    }
   ],
   "source": [
    "pred = gs.predict(X)\n",
    "print('RMSE for LASSO Model (alpha = 525):',\n",
    "      mean_squared_error(y, pred) ** .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('lasso', Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'lasso__max_iter': [10000], 'lasso__alpha': [501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': [x + 501 for x in range(50)]\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3) # CV = 3 for time\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8395976166452542\n",
      "{'lasso__alpha': 521, 'lasso__max_iter': 10000}\n",
      "0.8763544880844494\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for LASSO Model (alpha = 525): 26139.705802079166\n"
     ]
    }
   ],
   "source": [
    "pred = gs.predict(X)\n",
    "print('RMSE for LASSO Model (alpha = 525):',\n",
    "      mean_squared_error(y, pred) ** .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I don't know what exactly to expect, nor what range my alpha values can fall\n",
    "# in, so I'll cast a wide net and see what I catch.\n",
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': [0.1, 0.3, 0.5, 0.75, 1.0, 1.5, 2.0, 3.0, 4.0, 5.0,\n",
    "                    6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0,\n",
    "                    16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0,\n",
    "                    25.0, 26.0, 27.0, 29.0, 30.0]\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3) # CV = 3 for time\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO Scores - Given that the model is privy to the upper limit we tested\n",
    "# for alpha, it makes sense to keep going higher until we hit a ceiling.\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': range(30, 50, 1)\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score is improving slightly, however we are still at our upper limit\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': range(50, 100, 1)\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score has only improved marginally. Perhaps the scale is too small\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Run 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': range(100, 300, 3)\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long can this go on?\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso Run 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': range(300, 1000, 3)\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last we seem to have found the ballpark for an optimal alpha value!\n",
    "### On the first run through the code, the ideal alpha given was 413.9. It now\n",
    "### sits at 561. While it is expected for this value to change, the difference\n",
    "### is quite dramatic. I will rerun the code from the necessary spot to observe\n",
    "### to what extent it continues to vary.\n",
    "##### The third iteration has yielded an ideal alpha of 744! This warrants\n",
    "##### further examination\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lasso alpha refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso()\n",
    "lasso_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lasso', lasso)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clearly this was intended for the first run when the ideal alpha was in\n",
    "### the middle of the range given.\n",
    "lasso_params = {\n",
    "    'lasso__max_iter': [10000],\n",
    "    'lasso__alpha': [(x * 0.1) + 411 for x in range(50)] # To produce floats in range\n",
    "}\n",
    "gs = GridSearchCV(lasso_pipe, param_grid=lasso_params, cv=3)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A satisfactory alpha value (alpha = 413.9)\n",
    "### At least it was on the original run\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for LASSO Model (alpha = 413.9): 26147.046766428633\n"
     ]
    }
   ],
   "source": [
    "# Oddly enough this model produces a higher RMSE than the plain linear model\n",
    "### This has since rectified itself.\n",
    "pred = gs.predict(X)\n",
    "print('RMSE for LASSO Model (alpha = 413.9):',\n",
    "      mean_squared_error(y, pred) ** .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing First Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X equal to testing dataframe\n",
    "X = test\n",
    "\n",
    "# Predict target values\n",
    "pred = gs.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate first submission df\n",
    "sub_one = pd.DataFrame(data=pred, index=test['Id'])\n",
    "sub_one.columns = ['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate first submission df\n",
    "sub_two = pd.DataFrame(data=pred, index=test['Id'])\n",
    "sub_two.columns = ['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "sub_one.to_csv('./sub_one.csv') # RMSE = 37882.77012 (24th place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Second Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of curiosity, I wanted to see how the MLR performed. Below we wil rerun\n",
    "# the necessary code to regenerate the model.\n",
    "X = df[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "LinearRegression()\n",
    "lr_pipe = Pipeline([\n",
    "    ('ss', ss),\n",
    "    ('lr', lr),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {}\n",
    "gs = GridSearchCV(lr_pipe, lr_params, cv=10)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X equal to testing dataframe\n",
    "X = test\n",
    "\n",
    "# Predict target values\n",
    "pred = gs.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate second submission df\n",
    "sub_two = pd.DataFrame(data=pred, index=test['Id'])\n",
    "sub_two.columns = ['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "sub_two.to_csv('./sub_two.csv') # RMSE = 95203414630844.20000 # As it should be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While implementing LASSO as a means of feature selection hurt the interpretability of the model, it was successful in producing a viable model from an overly complex feature set. To further improve the model, I would integrate interaction terms with the polynomial features module and probably use elastic net to take advantage of ridge penalties. The use of pipelines reduces the implementation times of these features, however learning about them the day before the submission deadline, coupled with other unforeseen complications that have taken away from the time available to work on this project are limiting factors. That being said, this project was a fantastic learning experience; beyond figuring out how to make a workflow such as this work for Kaggle competitions, it became necessary to further understand how the LASSO model achieves what it does and how exactly the potential range of alpha values can vary so drastically across applications.\n",
    "\n",
    "###### Questions and tasks that remain:\n",
    "\n",
    "1) What is the effect of standardizing the dataset before generating dummy columns vs. after?  \n",
    "2) What criteria make for good interaction terms?  \n",
    "3a) When should one generate polynomial terms in addition to the interaction terms?  \n",
    "3b) What criteria determine what degree of the poly_terms we should generate?  \n",
    "4) How would elastic ridge perform against just LASSO in this case?  \n",
    "5) How can one determine what features were eliminated by LASSO rather than just having the function spit out a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
