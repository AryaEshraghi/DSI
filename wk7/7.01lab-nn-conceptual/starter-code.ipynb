{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lab\n",
    "\n",
    "In this lab, we'll be exploring a visual proof of the universal approximation theorem and building (from scratch) a neural network that will approximate a pretty ridiculous function.\n",
    "\n",
    "Head over to [this site](http://neuralnetworksanddeeplearning.com/chap4.html) and read from the beginning of the page until the \"Many Input Variables\" section. (You do not need to read the \"Many Input Variables\" section and beyond but are certainly welcome to do so!) You'll read the introduction, the \"Two Caveats\" section, and the \"Universality with One Input and One Output\" section.\n",
    "\n",
    "Your answers to problems 1-5 should come from directly this reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1**: Summarize the Universal Approximation Theorem. (Don't copy it; use your own words!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neural networks can compute any function so long as it is continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2**: Summarize the two caveats the author uses to describe the statement \"a neural network can compute any function.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Function must be continuous, and the result will always be approximate (obviously)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3:** For a sigmoidal activation function to closely resemble a step function, how would you describe the value of $w$? What constraints exist on the value of $b$? How do we calculate $s$? What does the value of $s$ indicate?\n",
    "\n",
    "Try playing around with the applets on the page to test how different parts of the perceptron affect the output. This should be helpful in answering the questions above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The value of w determines how 'compressed' the function appears, or to put it in different words, how quickly the slope changes. It is more compressed/the slope is steeper with a higher value of w\n",
    "\n",
    "There is a soft cap on its value after which it has no effect, and its value relative to w is what determines the step-ness of the curve\n",
    "\n",
    "The step is found through the inverse relationship between bias and weight. It can be formulaically expressed as s = -b/w\n",
    "\n",
    "s indicates where the step in a function can be found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4**: When the author wants us to approximate $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ with a neural network, the function on the applet where we manipulate the values of $h_i$ is not $f(x)$. It's a different function. What is this function, and why are we working with this one instead of $f(x)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The output of the NN (sigma) is not weighted such that we can easily determine the minimum of our loss function. By taking the inverse of sigma and applying it to f(x), we are then in a position to analyze and optimize our values for h to minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 5**: The author asks you to find values of $h_i$ that make your neural network closely approximate $\\sigma^{-1}\\circ f(x)$. Record your values of $h_i$ here and your best \"average deviation\" score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "h[0.0, 0.2) = -1.2\n",
    "\n",
    "h[0.2, 0.4) = -1.5\n",
    "\n",
    "h[0.4, 0.6) = -0.4\n",
    "\n",
    "h[0.6, 0.8) = -1.0\n",
    "\n",
    "h[0.8, 1.0] = 1.0\n",
    "\n",
    "Avg. dev = 0.38"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 6**: Build the neural network from your work in Problem 5 here.\n",
    "\n",
    "A few things to keep in mind:\n",
    "* How many inputs are there? \n",
    "* How many outputs are there?\n",
    "* How many neurons are in the hidden layer?\n",
    "* In order to create step functions between 0 and 0.2, 0.2 and 0.4, etc., what does this suggest about the activation function in these neurons? Note that these activation functions will be different, but related.\n",
    "* What do the values of $h_i$ represent?\n",
    "\n",
    "Check out the Neural Networks I notes for an implementation in NumPy; you should be able to use this as a starting point for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig_act(z):\n",
    "    if z > 6:\n",
    "        return(1)\n",
    "    if z < -6:\n",
    "        return(0)\n",
    "    return(1 / (1 + np.exp(-z)))\n",
    "\n",
    "def lin(x):\n",
    "    return(x)\n",
    "\n",
    "# s = -b/w (negative bias over weight)\n",
    "# w = 500\n",
    "def solve_for_bias(s, w=500):\n",
    "    return(-w / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "bias = np.array([solve_for_bias(n) for n in steps])\n",
    "weights = np.array([500] * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_output = np.array([-1.1, 1.1, -1.4, 1.4, -0.4, 0.4, -1, 1, 1.1, -1.1])\n",
    "output_bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(x, activation_function = lin):\n",
    "    node_vals = x * weights * bias\n",
    "    \n",
    "    activation_vals = np.array([sig_act(z) for z in node_vals])\n",
    "    \n",
    "    output = np.sum(activation_vals * weights_output) + output_bias\n",
    "    \n",
    "    activation_output = activation_function(output)\n",
    "    \n",
    "    return(activation_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.linspace(0, 1, 1000)\n",
    "y_vals = [network(x) for x in x_vals]\n",
    "\n",
    "def f(x):\n",
    "    return(.2 + .4 * (x ** 2) + 3 * x * np.sin(15 * x) + .05 * np.cos(50 * x))\n",
    "\n",
    "y_true = [f(x) for x in x_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)=0.2+0.4x2+0.3xsin(15x)+0.05cos(50x),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 7**: Once you've built the neural network, use `np.linspace` to generate 1000 values of $x$ between 0 and 1 and use the `pynverse` [library](https://pypi.python.org/pypi/pynverse) to manually estimate the performance of your neural network using mean squared error.\n",
    "\n",
    "Recall that mean squared error is given by:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum_{i=1}^n (\\hat{y}-y)^2\n",
    "$$\n",
    "\n",
    "\n",
    "* Your $\\hat{y}$ in this case are your predicted values from your neural network for each of the $x$ that you generated using `np.linspace`. Make sure to take into account the final activation function!\n",
    "* Your $y$ values are the actual observed values of $f(x)=0.2+0.4x^2+0.3x\\sin(15x)+0.05\\cos(50x)$ for each of the $x$ that you generated using `np.linspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynverse import inversefunc\n",
    "inv_sigmoid = inversefunc(sig_act)\n",
    "\n",
    "y_hat = [network(x) for x in x_vals]\n",
    "y_invsig = [inv_sigmoid(i) for i in y_true]\n",
    "plt.plot(x_vals, y_invsig)\n",
    "plt.plot(x_vals, y_hat)\n",
    "plt.xlim((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_MSE(y_hat, y):\n",
    "    if len(y_hat) != len(y):\n",
    "        print('you dun goofed')\n",
    "        return()\n",
    "    n = len(y_hat)\n",
    "    SE = [(y_hat[i] - y[i]) ** 2 for i in range(n)]\n",
    "    MSE = sum(SE) / n\n",
    "    return(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_MSE(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 8**: Suppose you wanted to increase the performance of this neural network. How might you go about doing so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perhaps reducing the number of nodes would improve performance at the cost of speed of convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
