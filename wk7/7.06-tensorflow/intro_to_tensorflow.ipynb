{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Neural Networks with TensorFlow\n",
    "\n",
    "_Authors: Justin Pounders (ATL) and Riley Dalles (ATX)_\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Describe the basic `tensorflow` workflow.\n",
    "- Create computational graphs representing basic feed-forward neural networks.\n",
    "- Train neural networks using `tensorflow`\n",
    "- Create and train neural networks for both regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs\n",
    "\n",
    "---\n",
    "\n",
    "Tensorflow is fundamentally a library for creating **computational graphs**.\n",
    "\n",
    "![](assets/comp_graph.png)\n",
    "\n",
    "Let's define and evaluate this \"computational graph.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are two phases to building a `tensorflow` model.**\n",
    "\n",
    "1. Graph construction\n",
    "2. Training/evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "a = tf.Variable(3, name='a')\n",
    "b = tf.Variable(4, name='b')\n",
    "c = a + b\n",
    "d = b + 1\n",
    "e = c * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the graph\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = sess.run(e)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Regression\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a neural net for _regression_.  These are the steps we will follow:\n",
    "\n",
    "1. Load the data.\n",
    "2. Data cleaning/munging, feature engineering (will not do today)\n",
    "3. Make test/train splits. (Should we use cross validation?)\n",
    "4. Standardize the data.\n",
    "5. Build the computational graph for the neural network.\n",
    "6. Train the network using gradient descent a.k.a. back propogation.\n",
    "7. Evaluate performance and iterate.\n",
    "\n",
    "For regression, we will have one output unit (neuron) with _no_ activation function.  The value of this outpu unit will be prediction of the network given whatever input values went into the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 1-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data = fetch_california_housing()\n",
    "# data.data is the data matrix (input features)\n",
    "# data.target is the label vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "ss = StandardScaler()\n",
    "\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf needs matrix for y. For single value y's, they need to be reshaped\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Build the network/graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15480, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, (None, 8), name='Features')\n",
    "y = tf.placeholder(tf.float32, (None, 1), name='Target')\n",
    "\n",
    "h1 = tf.layers.dense(X, 8, tf.nn.relu)\n",
    "y_hat = tf.layers.dense(h1, 1)\n",
    "\n",
    "loss = tf.losses.mean_squared_error(y, y_hat)\n",
    "optimizer = tf.train.AdamOptimizer(.015)\n",
    "training_run = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss 3.5064144 test loss 3.4106135\n",
      "epoch 1 train loss 3.1257293 test loss 3.0426455\n",
      "epoch 2 train loss 2.776522 test loss 2.7043893\n",
      "epoch 3 train loss 2.4614332 test loss 2.3985877\n",
      "epoch 4 train loss 2.1817255 test loss 2.1265996\n",
      "epoch 5 train loss 1.9379259 test loss 1.8891773\n",
      "epoch 6 train loss 1.7304809 test loss 1.6859015\n",
      "epoch 7 train loss 1.5587207 test loss 1.5161966\n",
      "epoch 8 train loss 1.421239 test loss 1.3785329\n",
      "epoch 9 train loss 1.3139273 test loss 1.2700068\n",
      "epoch 10 train loss 1.232508 test loss 1.1870204\n",
      "epoch 11 train loss 1.1722637 test loss 1.1254729\n",
      "epoch 12 train loss 1.1281993 test loss 1.0806235\n",
      "epoch 13 train loss 1.0957924 test loss 1.0478612\n",
      "epoch 14 train loss 1.071147 test loss 1.0232182\n",
      "epoch 15 train loss 1.0509272 test loss 1.0037255\n",
      "epoch 16 train loss 1.0327401 test loss 0.9871667\n",
      "epoch 17 train loss 1.0151297 test loss 0.9719866\n",
      "epoch 18 train loss 0.9973737 test loss 0.9572245\n",
      "epoch 19 train loss 0.97924876 test loss 0.94238234\n",
      "epoch 20 train loss 0.96094674 test loss 0.9278312\n",
      "epoch 21 train loss 0.94265395 test loss 0.9139907\n",
      "epoch 22 train loss 0.9248847 test loss 0.90072143\n",
      "epoch 23 train loss 0.9081582 test loss 0.8883411\n",
      "epoch 24 train loss 0.89287496 test loss 0.87689775\n",
      "epoch 25 train loss 0.87869483 test loss 0.86639714\n",
      "epoch 26 train loss 0.8657132 test loss 0.85676193\n",
      "epoch 27 train loss 0.854069 test loss 0.84789264\n",
      "epoch 28 train loss 0.84372026 test loss 0.839745\n",
      "epoch 29 train loss 0.8342544 test loss 0.8321991\n",
      "epoch 30 train loss 0.82527614 test loss 0.824874\n",
      "epoch 31 train loss 0.81656975 test loss 0.8174654\n",
      "epoch 32 train loss 0.8078358 test loss 0.80970997\n",
      "epoch 33 train loss 0.7989857 test loss 0.80143094\n",
      "epoch 34 train loss 0.7899392 test loss 0.79250115\n",
      "epoch 35 train loss 0.7806956 test loss 0.78291917\n",
      "epoch 36 train loss 0.77136046 test loss 0.77288264\n",
      "epoch 37 train loss 0.7620039 test loss 0.7626376\n",
      "epoch 38 train loss 0.7528951 test loss 0.7524487\n",
      "epoch 39 train loss 0.744205 test loss 0.7425337\n",
      "epoch 40 train loss 0.73609775 test loss 0.7331228\n",
      "epoch 41 train loss 0.7286921 test loss 0.72443235\n",
      "epoch 42 train loss 0.722028 test loss 0.7165582\n",
      "epoch 43 train loss 0.7160399 test loss 0.7094944\n",
      "epoch 44 train loss 0.7105899 test loss 0.70316964\n",
      "epoch 45 train loss 0.70550346 test loss 0.6974397\n",
      "epoch 46 train loss 0.70059854 test loss 0.6920745\n",
      "epoch 47 train loss 0.6956695 test loss 0.6868753\n",
      "epoch 48 train loss 0.69055396 test loss 0.6817121\n",
      "epoch 49 train loss 0.68514395 test loss 0.67646426\n",
      "epoch 50 train loss 0.6794401 test loss 0.67109203\n",
      "epoch 51 train loss 0.67350185 test loss 0.6656193\n",
      "epoch 52 train loss 0.6674195 test loss 0.66009873\n",
      "epoch 53 train loss 0.661311 test loss 0.65463287\n",
      "epoch 54 train loss 0.65531105 test loss 0.6493107\n",
      "epoch 55 train loss 0.64951146 test loss 0.6441752\n",
      "epoch 56 train loss 0.6439422 test loss 0.6392897\n",
      "epoch 57 train loss 0.638619 test loss 0.63464874\n",
      "epoch 58 train loss 0.63353074 test loss 0.63019687\n",
      "epoch 59 train loss 0.62863487 test loss 0.62590116\n",
      "epoch 60 train loss 0.6238865 test loss 0.6216514\n",
      "epoch 61 train loss 0.619219 test loss 0.61743546\n",
      "epoch 62 train loss 0.6145607 test loss 0.61318195\n",
      "epoch 63 train loss 0.60988396 test loss 0.6088551\n",
      "epoch 64 train loss 0.6052818 test loss 0.6044639\n",
      "epoch 65 train loss 0.60064536 test loss 0.59996206\n",
      "epoch 66 train loss 0.5959437 test loss 0.59537053\n",
      "epoch 67 train loss 0.5913621 test loss 0.5907858\n",
      "epoch 68 train loss 0.5868648 test loss 0.5862648\n",
      "epoch 69 train loss 0.5824389 test loss 0.58182096\n",
      "epoch 70 train loss 0.5781134 test loss 0.57747597\n",
      "epoch 71 train loss 0.57390594 test loss 0.5732912\n",
      "epoch 72 train loss 0.56989837 test loss 0.56924176\n",
      "epoch 73 train loss 0.5659755 test loss 0.56530994\n",
      "epoch 74 train loss 0.5621074 test loss 0.5614973\n",
      "epoch 75 train loss 0.55826104 test loss 0.55776185\n",
      "epoch 76 train loss 0.5544296 test loss 0.55408907\n",
      "epoch 77 train loss 0.5506022 test loss 0.5504654\n",
      "epoch 78 train loss 0.5467749 test loss 0.54686034\n",
      "epoch 79 train loss 0.5429613 test loss 0.5432992\n",
      "epoch 80 train loss 0.5391913 test loss 0.5397714\n",
      "epoch 81 train loss 0.53547645 test loss 0.536289\n",
      "epoch 82 train loss 0.5318515 test loss 0.53284806\n",
      "epoch 83 train loss 0.52831703 test loss 0.5294991\n",
      "epoch 84 train loss 0.5248259 test loss 0.5262388\n",
      "epoch 85 train loss 0.52139276 test loss 0.52302104\n",
      "epoch 86 train loss 0.5180157 test loss 0.51982564\n",
      "epoch 87 train loss 0.5147161 test loss 0.51665604\n",
      "epoch 88 train loss 0.511471 test loss 0.51347154\n",
      "epoch 89 train loss 0.50827724 test loss 0.51030093\n",
      "epoch 90 train loss 0.505129 test loss 0.50716996\n",
      "epoch 91 train loss 0.5020243 test loss 0.50410324\n",
      "epoch 92 train loss 0.49897352 test loss 0.5010747\n",
      "epoch 93 train loss 0.49598134 test loss 0.49812087\n",
      "epoch 94 train loss 0.49306622 test loss 0.4952387\n",
      "epoch 95 train loss 0.4902654 test loss 0.49242663\n",
      "epoch 96 train loss 0.48755533 test loss 0.48973\n",
      "epoch 97 train loss 0.4849592 test loss 0.48714024\n",
      "epoch 98 train loss 0.4825061 test loss 0.48464927\n",
      "epoch 99 train loss 0.48012424 test loss 0.4822262\n",
      "epoch 100 train loss 0.47777298 test loss 0.47990033\n",
      "epoch 101 train loss 0.47544402 test loss 0.47765437\n",
      "epoch 102 train loss 0.473169 test loss 0.47549018\n",
      "epoch 103 train loss 0.4709501 test loss 0.473411\n",
      "epoch 104 train loss 0.4687887 test loss 0.47140005\n",
      "epoch 105 train loss 0.4666963 test loss 0.4694659\n",
      "epoch 106 train loss 0.4646837 test loss 0.46759516\n",
      "epoch 107 train loss 0.46274537 test loss 0.46578416\n",
      "epoch 108 train loss 0.46088922 test loss 0.46404538\n",
      "epoch 109 train loss 0.45908165 test loss 0.46238646\n",
      "epoch 110 train loss 0.45738226 test loss 0.4608179\n",
      "epoch 111 train loss 0.4557925 test loss 0.4593308\n",
      "epoch 112 train loss 0.45429188 test loss 0.45796323\n",
      "epoch 113 train loss 0.45287904 test loss 0.4566896\n",
      "epoch 114 train loss 0.4515425 test loss 0.45546457\n",
      "epoch 115 train loss 0.4502416 test loss 0.45428136\n",
      "epoch 116 train loss 0.4489728 test loss 0.45314458\n",
      "epoch 117 train loss 0.44774926 test loss 0.45205268\n",
      "epoch 118 train loss 0.44661474 test loss 0.451023\n",
      "epoch 119 train loss 0.4455232 test loss 0.45004645\n",
      "epoch 120 train loss 0.44447562 test loss 0.44913065\n",
      "epoch 121 train loss 0.44346547 test loss 0.44824946\n",
      "epoch 122 train loss 0.4425022 test loss 0.4474005\n",
      "epoch 123 train loss 0.44158792 test loss 0.44658893\n",
      "epoch 124 train loss 0.4407048 test loss 0.4458146\n",
      "epoch 125 train loss 0.43985668 test loss 0.4450657\n",
      "epoch 126 train loss 0.43906105 test loss 0.44434977\n",
      "epoch 127 train loss 0.4383116 test loss 0.4436631\n",
      "epoch 128 train loss 0.43759495 test loss 0.4430034\n",
      "epoch 129 train loss 0.4369214 test loss 0.4423722\n",
      "epoch 130 train loss 0.43626392 test loss 0.4417797\n",
      "epoch 131 train loss 0.43562612 test loss 0.4412141\n",
      "epoch 132 train loss 0.43501136 test loss 0.44066265\n",
      "epoch 133 train loss 0.43441764 test loss 0.44013804\n",
      "epoch 134 train loss 0.4338382 test loss 0.43964052\n",
      "epoch 135 train loss 0.4332722 test loss 0.439159\n",
      "epoch 136 train loss 0.4327332 test loss 0.43868873\n",
      "epoch 137 train loss 0.43221036 test loss 0.43821573\n",
      "epoch 138 train loss 0.43169093 test loss 0.43773714\n",
      "epoch 139 train loss 0.43117425 test loss 0.4372463\n",
      "epoch 140 train loss 0.43066224 test loss 0.4367576\n",
      "epoch 141 train loss 0.43015844 test loss 0.436271\n",
      "epoch 142 train loss 0.4296617 test loss 0.4357891\n",
      "epoch 143 train loss 0.42917398 test loss 0.4353176\n",
      "epoch 144 train loss 0.42869517 test loss 0.43485656\n",
      "epoch 145 train loss 0.42823198 test loss 0.43440136\n",
      "epoch 146 train loss 0.42778328 test loss 0.43395546\n",
      "epoch 147 train loss 0.42733887 test loss 0.4335232\n",
      "epoch 148 train loss 0.42690146 test loss 0.43310145\n",
      "epoch 149 train loss 0.42647374 test loss 0.43268627\n",
      "epoch 150 train loss 0.4260536 test loss 0.4322836\n",
      "epoch 151 train loss 0.4256442 test loss 0.43188864\n",
      "epoch 152 train loss 0.42524415 test loss 0.43149728\n",
      "epoch 153 train loss 0.42485115 test loss 0.43110538\n",
      "epoch 154 train loss 0.42446372 test loss 0.43071643\n",
      "epoch 155 train loss 0.4240826 test loss 0.43033305\n",
      "epoch 156 train loss 0.42370707 test loss 0.42995417\n",
      "epoch 157 train loss 0.42333737 test loss 0.429578\n",
      "epoch 158 train loss 0.42297596 test loss 0.42920575\n",
      "epoch 159 train loss 0.42262 test loss 0.42883575\n",
      "epoch 160 train loss 0.42227054 test loss 0.42846513\n",
      "epoch 161 train loss 0.42192417 test loss 0.42809972\n",
      "epoch 162 train loss 0.42158103 test loss 0.42774442\n",
      "epoch 163 train loss 0.42124674 test loss 0.42739472\n",
      "epoch 164 train loss 0.42091674 test loss 0.4270509\n",
      "epoch 165 train loss 0.420588 test loss 0.42670813\n",
      "epoch 166 train loss 0.42026842 test loss 0.42636955\n",
      "epoch 167 train loss 0.41995308 test loss 0.42604157\n",
      "epoch 168 train loss 0.41964048 test loss 0.42572314\n",
      "epoch 169 train loss 0.41933003 test loss 0.4254149\n",
      "epoch 170 train loss 0.41902286 test loss 0.42511484\n",
      "epoch 171 train loss 0.41872126 test loss 0.42482275\n",
      "epoch 172 train loss 0.418422 test loss 0.4245318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 173 train loss 0.41812542 test loss 0.4242424\n",
      "epoch 174 train loss 0.4178335 test loss 0.42395833\n",
      "epoch 175 train loss 0.41754565 test loss 0.4236745\n",
      "epoch 176 train loss 0.41725993 test loss 0.4233951\n",
      "epoch 177 train loss 0.41697693 test loss 0.42311928\n",
      "epoch 178 train loss 0.41669858 test loss 0.42284313\n",
      "epoch 179 train loss 0.41642728 test loss 0.4225665\n",
      "epoch 180 train loss 0.4161582 test loss 0.42228606\n",
      "epoch 181 train loss 0.4158897 test loss 0.42200473\n",
      "epoch 182 train loss 0.41562718 test loss 0.4217224\n",
      "epoch 183 train loss 0.4153663 test loss 0.42144275\n",
      "epoch 184 train loss 0.41510874 test loss 0.4211727\n",
      "epoch 185 train loss 0.41485208 test loss 0.42091244\n",
      "epoch 186 train loss 0.41459733 test loss 0.42065847\n",
      "epoch 187 train loss 0.4143462 test loss 0.4204092\n",
      "epoch 188 train loss 0.41409892 test loss 0.42016023\n",
      "epoch 189 train loss 0.41385543 test loss 0.4199107\n",
      "epoch 190 train loss 0.41361836 test loss 0.41965955\n",
      "epoch 191 train loss 0.4133827 test loss 0.41940588\n",
      "epoch 192 train loss 0.41315046 test loss 0.41915008\n",
      "epoch 193 train loss 0.41292292 test loss 0.418898\n",
      "epoch 194 train loss 0.41269764 test loss 0.41865054\n",
      "epoch 195 train loss 0.4124747 test loss 0.41840526\n",
      "epoch 196 train loss 0.41225517 test loss 0.41816384\n",
      "epoch 197 train loss 0.4120379 test loss 0.4179272\n",
      "epoch 198 train loss 0.41181993 test loss 0.41768798\n",
      "epoch 199 train loss 0.41160235 test loss 0.41744187\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train, y: y_train})\n",
    "        train_losses.append(train_loss)\n",
    "        test_loss = sess.run(loss, feed_dict={X: X_test, y: y_test})\n",
    "        test_losses.append(test_loss)\n",
    "        print('epoch', epoch, 'train loss', train_loss, 'test loss', test_loss)\n",
    "\n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6859853169271473"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate r^2\n",
    "metrics.r2_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a28662ba8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curves\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Binary Classification\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build a neural net for _classification_.  We will follow the same steps as for regression:\n",
    "\n",
    "1. Load the data.\n",
    "2. Data cleaning/munging, feature engineering (will not do today)\n",
    "3. Make test/train splits. (Should we use cross validation?)\n",
    "4. Standardize the data.\n",
    "5. Build the computational graph for the neural network.\n",
    "6. Train the network using gradient descent a.k.a. back propogation.\n",
    "7. Evaluate performance and iterate.\n",
    "\n",
    "For _binary classification_ we will have one output unit that will represent the **probability** of \"class 1.\"  Because we want a probability as output, we need to select an activation function that yields values between 0 and 1, i.e., sigmoid function or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "features, labels = make_classification(n_samples=10000, n_features=20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels)\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)\n",
    "#in data.target...\n",
    "#  1 = benign\n",
    "#  0 = malignant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(-1, 1)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, (None, 20))\n",
    "y = tf.placeholder(tf.float32, (None, 1))\n",
    "\n",
    "h1 = tf.layers.dense(X, 20, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, 12, tf.nn.relu)\n",
    "y_hat = tf.layers.dense(h2, 1, tf.nn.sigmoid) # probs, sigmoid for single class (softmax for multi)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat) # binary crossentropy\n",
    "optimizer = tf.train.AdamOptimizer(.015)\n",
    "training_run = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epochs in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (pred > .5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9328"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, (None, 20))\n",
    "y = tf.placeholder(tf.float32, (None, 1))\n",
    "\n",
    "h1 = tf.layers.dense(X, 20, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, 12, tf.nn.relu)\n",
    "y_hat = tf.layers.dense(h2, 1, tf.nn.sigmoid) # probs, sigmoid for single class (softmax for multi)\n",
    "\n",
    "loss = tf.losses.log_loss(y, y_hat) # binary crossentropy\n",
    "optimizer = tf.train.AdamOptimizer(.015)\n",
    "training_run = optimizer.minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "        \n",
    "    saver.save(sess, './classification-model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, './classification-model.ckpt')\n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})\n",
    "\n",
    "classes = (pred > .5).astype(int)\n",
    "metrics.accuracy_score(y_test, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, let's go back and add a new hidden layer to our network..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the input\n",
    "\n",
    "What if we can't store all of the training data in memory?\n",
    "\n",
    "We can split the data up into \"batches\" and feed them to the network one at a time.  This just means that we splitting up the data and feeding it to the network one piece at a time.\n",
    "\n",
    "You could write your own function to dole out subsets of the data one at a time, or you could use this creative hack to get `sklearn` to do it for you.  (Thanks, Riley, for showing me this!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_batches = round(X_train.shape[0] / batch_size)\n",
    "\n",
    "kf = KFold(n_splits=num_batches, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  36,  117,  167,  170,  185,  213,  274,  347,  463,  540,  779,\n",
       "         784,  819,  961, 1003, 1010, 1162, 1174, 1227, 1273, 1297, 1326,\n",
       "        1413, 1504, 1554, 1564, 1612, 1673, 1795, 1845, 2044, 2091, 2095,\n",
       "        2111, 2114, 2187, 2203, 2221, 2322, 2486, 2580, 2710, 2870, 3022,\n",
       "        3089, 3238, 3279, 3346, 3356, 3363, 3413, 3460, 3658, 3692, 3732,\n",
       "        3750, 4032, 4050, 4066, 4350, 4549, 4565, 4575, 4592, 4736, 4996,\n",
       "        5006, 5018, 5177, 5221, 5327, 5329, 5361, 5403, 5408, 5544, 5597,\n",
       "        5612, 5644, 5660, 5733, 5813, 5894, 5913, 6014, 6016, 6072, 6127,\n",
       "        6159, 6208, 6507, 6603, 6613, 6720, 6812, 6977, 7026, 7052, 7279,\n",
       "        7491]),\n",
       " array([  52,  102,  140,  147,  152,  308,  317,  323,  363,  400,  420,\n",
       "         441,  467,  594,  600, 1015, 1108, 1183, 1193, 1224, 1254, 1371,\n",
       "        1373, 1581, 1676, 1678, 1754, 1895, 1899, 1967, 2051, 2093, 2109,\n",
       "        2202, 2307, 2387, 2487, 2538, 2573, 2582, 2656, 2693, 2779, 2864,\n",
       "        2901, 2963, 3086, 3125, 3170, 3191, 3239, 3248, 3301, 3331, 3333,\n",
       "        3549, 3567, 3795, 3891, 3997, 4160, 4174, 4385, 4419, 4545, 4590,\n",
       "        4756, 4810, 4934, 5083, 5150, 5244, 5312, 5348, 5477, 5509, 5578,\n",
       "        5605, 5610, 5652, 5673, 5782, 5815, 5857, 5874, 5898, 5984, 6301,\n",
       "        6305, 6400, 6556, 6579, 6672, 6702, 6764, 6847, 7002, 7225, 7226,\n",
       "        7340]),\n",
       " array([   5,   56,  149,  287,  289,  459,  478,  480,  498,  518,  524,\n",
       "         599,  825,  916,  918,  962,  963,  989, 1071, 1082, 1113, 1132,\n",
       "        1143, 1176, 1277, 1300, 1391, 1789, 1903, 1949, 2001, 2025, 2166,\n",
       "        2240, 2305, 2454, 2455, 2540, 2617, 2702, 2736, 2829, 2832, 2938,\n",
       "        2992, 3172, 3246, 3250, 3309, 3310, 3468, 3637, 4014, 4181, 4409,\n",
       "        4484, 4509, 4676, 4770, 4822, 4841, 4883, 4949, 5061, 5069, 5116,\n",
       "        5140, 5160, 5238, 5282, 5349, 5440, 5471, 5491, 5523, 5713, 5768,\n",
       "        5831, 5844, 5952, 6030, 6121, 6147, 6349, 6416, 6426, 6442, 6529,\n",
       "        6530, 6538, 6641, 6787, 6823, 6890, 6892, 6911, 7127, 7331, 7392,\n",
       "        7485]),\n",
       " array([   1,   87,   92,  175,  198,  229,  339,  489,  523,  576,  587,\n",
       "         632,  693,  744, 1319, 1417, 1482, 1501, 1510, 1652, 1747, 1751,\n",
       "        1780, 1807, 2024, 2030, 2041, 2131, 2236, 2241, 2365, 2382, 2426,\n",
       "        2476, 2575, 2589, 2596, 2611, 2726, 2773, 2815, 2898, 2956, 2993,\n",
       "        2996, 3189, 3204, 3302, 3760, 3785, 3801, 3824, 3840, 3857, 4034,\n",
       "        4084, 4133, 4216, 4283, 4284, 4332, 4470, 4516, 4638, 4725, 4811,\n",
       "        4844, 4993, 5025, 5110, 5323, 5324, 5648, 5737, 5842, 5861, 6024,\n",
       "        6037, 6058, 6141, 6336, 6354, 6357, 6524, 6534, 6561, 6785, 6854,\n",
       "        6937, 7049, 7077, 7093, 7115, 7120, 7122, 7186, 7212, 7218, 7437,\n",
       "        7477]),\n",
       " array([  22,   28,   55,  181,  220,  236,  266,  413,  581,  595,  716,\n",
       "         770,  852,  905,  917,  965, 1272, 1285, 1381, 1498, 1715, 1719,\n",
       "        1728, 1835, 2005, 2100, 2104, 2119, 2145, 2180, 2319, 2334, 2740,\n",
       "        2778, 2780, 2801, 2814, 2830, 2943, 3101, 3122, 3166, 3197, 3209,\n",
       "        3319, 3327, 3473, 3493, 3502, 3564, 3583, 3810, 3913, 3954, 3964,\n",
       "        4077, 4166, 4171, 4252, 4296, 4369, 4375, 4412, 4615, 4702, 4763,\n",
       "        4873, 4950, 4952, 4968, 5011, 5103, 5129, 5155, 5436, 5450, 5458,\n",
       "        5584, 5590, 5639, 5781, 5873, 5946, 6025, 6133, 6224, 6445, 6486,\n",
       "        6681, 6712, 6758, 6845, 6863, 6898, 7121, 7316, 7431, 7433, 7448,\n",
       "        7493]),\n",
       " array([  71,  103,  106,  180,  408,  447,  536,  538,  541,  579,  736,\n",
       "         846,  942, 1270, 1309, 1471, 1604, 1634, 1755, 1786, 1830, 1851,\n",
       "        1888, 1937, 1953, 1966, 1972, 2032, 2043, 2075, 2123, 2124, 2126,\n",
       "        2199, 2270, 2483, 2606, 2642, 2645, 2665, 2742, 2770, 2775, 2934,\n",
       "        3021, 3128, 3256, 3305, 3521, 3524, 3539, 3542, 3744, 3768, 3859,\n",
       "        4339, 4345, 4408, 4528, 4535, 4639, 4737, 4743, 4752, 4839, 5005,\n",
       "        5008, 5076, 5098, 5483, 5598, 5631, 5635, 5666, 5723, 5730, 5934,\n",
       "        5958, 5964, 6000, 6128, 6140, 6250, 6270, 6308, 6340, 6527, 6635,\n",
       "        6676, 6696, 6750, 6840, 6938, 6946, 6981, 6992, 7043, 7206, 7209,\n",
       "        7296]),\n",
       " array([  12,   34,  136,  148,  513,  525,  547,  570,  785,  801,  817,\n",
       "         828,  862, 1013, 1020, 1036, 1060, 1078, 1096, 1177, 1349, 1390,\n",
       "        1481, 1616, 1649, 1831, 1859, 2045, 2078, 2209, 2218, 2341, 2366,\n",
       "        2465, 2533, 2638, 2796, 2827, 2860, 3116, 3159, 3203, 3264, 3312,\n",
       "        3345, 3674, 3696, 3734, 3844, 3883, 3905, 3932, 3941, 3989, 4039,\n",
       "        4106, 4148, 4427, 4473, 4482, 4548, 4705, 4759, 4860, 5043, 5093,\n",
       "        5174, 5204, 5267, 5273, 5280, 5335, 5426, 5496, 5601, 5858, 5875,\n",
       "        5973, 6094, 6161, 6184, 6254, 6258, 6269, 6376, 6387, 6398, 6424,\n",
       "        6523, 6528, 6557, 6585, 6650, 6724, 6919, 7020, 7306, 7318, 7341,\n",
       "        7419]),\n",
       " array([ 246,  273,  360,  364,  384,  406,  560,  567,  666,  683,  731,\n",
       "         732,  843, 1077, 1129, 1178, 1331, 1683, 1693, 1738, 1750, 1778,\n",
       "        1818, 1836, 1923, 1952, 2195, 2254, 2258, 2303, 2340, 2386, 2393,\n",
       "        2485, 2724, 2806, 2820, 2838, 2851, 2874, 2900, 2941, 3039, 3071,\n",
       "        3131, 3178, 3211, 3213, 3263, 3417, 3591, 3619, 3765, 3855, 3921,\n",
       "        4064, 4230, 4308, 4315, 4389, 4515, 4699, 4707, 4713, 4782, 4866,\n",
       "        4933, 5015, 5033, 5094, 5130, 5154, 5183, 5300, 5393, 5497, 5728,\n",
       "        5772, 5802, 5942, 5975, 6062, 6090, 6240, 6247, 6353, 6423, 6427,\n",
       "        6509, 6543, 6715, 6728, 6976, 7075, 7140, 7149, 7287, 7408, 7425,\n",
       "        7461]),\n",
       " array([ 100,  206,  590,  614,  654,  679,  702,  719,  758,  763,  874,\n",
       "         884,  976, 1255, 1266, 1316, 1323, 1685, 1697, 1703, 1779, 1811,\n",
       "        1817, 1916, 2033, 2099, 2105, 2141, 2172, 2207, 2404, 2422, 2678,\n",
       "        2762, 2855, 3066, 3091, 3150, 3242, 3300, 3318, 3429, 3442, 3483,\n",
       "        3518, 3535, 3541, 3753, 3825, 3865, 3901, 4061, 4072, 4078, 4144,\n",
       "        4154, 4377, 4424, 4429, 4443, 4614, 4646, 4787, 4795, 4801, 4956,\n",
       "        5106, 5172, 5248, 5259, 5274, 5316, 5455, 5479, 5562, 5572, 5592,\n",
       "        5600, 5819, 5853, 5865, 5989, 6028, 6142, 6299, 6310, 6522, 6555,\n",
       "        6598, 6632, 6645, 6774, 6844, 6878, 6908, 7015, 7319, 7368, 7413,\n",
       "        7465]),\n",
       " array([  14,   49,  158,  247,  388,  411,  598,  747,  768,  914, 1095,\n",
       "        1100, 1107, 1195, 1260, 1389, 1488, 1541, 1544, 1561, 1589, 1628,\n",
       "        1670, 1700, 1774, 1868, 2010, 2206, 2260, 2330, 2501, 2511, 2570,\n",
       "        2841, 3057, 3160, 3212, 3366, 3367, 3545, 3562, 3818, 3862, 3950,\n",
       "        4163, 4218, 4390, 4472, 4492, 4514, 4520, 4595, 4635, 4745, 4803,\n",
       "        4964, 5108, 5364, 5401, 5425, 5453, 5484, 5570, 5596, 5623, 5700,\n",
       "        5719, 5895, 5924, 5955, 5994, 6084, 6164, 6234, 6279, 6288, 6294,\n",
       "        6348, 6436, 6458, 6489, 6514, 6574, 6599, 6630, 6647, 6791, 6837,\n",
       "        6849, 6877, 6889, 6984, 7033, 7369, 7387, 7427, 7455, 7456, 7479,\n",
       "        7495]),\n",
       " array([  51,  207,  249,  269,  359,  446,  448,  495,  515,  766,  796,\n",
       "         841,  988,  993, 1005, 1049, 1140, 1204, 1267, 1311, 1314, 1321,\n",
       "        1434, 1462, 1520, 1543, 1590, 1640, 1802, 1824, 1974, 2014, 2020,\n",
       "        2056, 2068, 2159, 2191, 2414, 2523, 2624, 2715, 2811, 2843, 2890,\n",
       "        3025, 3045, 3145, 3182, 3208, 3245, 3636, 3639, 3683, 4198, 4200,\n",
       "        4201, 4202, 4280, 4314, 4611, 4637, 4648, 4710, 4775, 4799, 4852,\n",
       "        4904, 5117, 5276, 5355, 5385, 5490, 5504, 5506, 5591, 5629, 5691,\n",
       "        5695, 5706, 5851, 5859, 5910, 5981, 6202, 6255, 6366, 6371, 6466,\n",
       "        6515, 6562, 6614, 6665, 6671, 6760, 6818, 7123, 7175, 7265, 7329,\n",
       "        7420]),\n",
       " array([  76,  228,  294,  407,  618,  668,  680,  699,  745,  748,  755,\n",
       "         816, 1048, 1069, 1249, 1287, 1463, 1496, 1553, 1586, 1626, 1694,\n",
       "        1735, 1739, 1743, 1833, 1880, 1965, 1985, 2052, 2151, 2215, 2255,\n",
       "        2271, 2384, 2424, 2586, 2602, 2620, 2629, 2835, 2844, 2927, 2974,\n",
       "        3162, 3198, 3200, 3206, 3320, 3335, 3365, 3475, 3557, 3727, 4073,\n",
       "        4348, 4456, 4462, 4501, 4568, 4580, 4626, 4665, 4696, 4721, 4830,\n",
       "        5072, 5073, 5164, 5182, 5189, 5285, 5352, 5386, 5930, 5974, 6017,\n",
       "        6023, 6099, 6191, 6196, 6265, 6284, 6287, 6553, 6649, 6727, 6756,\n",
       "        6900, 6972, 6999, 7054, 7059, 7072, 7078, 7146, 7166, 7204, 7230,\n",
       "        7489]),\n",
       " array([  15,  237,  259,  352,  578,  631,  764,  806,  812,  887,  950,\n",
       "         978, 1123, 1199, 1214, 1298, 1348, 1379, 1525, 1535, 1705, 1708,\n",
       "        1813, 1847, 1935, 2002, 2034, 2060, 2210, 2375, 2453, 2558, 2615,\n",
       "        2675, 2706, 2721, 2746, 2785, 2915, 3095, 3129, 3146, 3148, 3214,\n",
       "        3405, 3450, 3677, 3842, 3875, 4009, 4036, 4139, 4294, 4461, 4476,\n",
       "        4562, 4594, 4642, 4650, 4888, 5075, 5122, 5137, 5184, 5193, 5286,\n",
       "        5488, 5656, 5658, 5696, 5722, 5743, 5804, 5867, 5891, 5900, 5904,\n",
       "        5912, 6105, 6372, 6482, 6487, 6513, 6604, 6731, 6763, 6830, 6880,\n",
       "        6886, 6893, 6968, 7006, 7163, 7165, 7261, 7276, 7339, 7360, 7366,\n",
       "        7466]),\n",
       " array([  10,   93,  177,  210,  272,  300,  306,  375,  383,  471,  486,\n",
       "         517,  661,  772,  813,  818,  954, 1007, 1105, 1194, 1402, 1487,\n",
       "        1503, 1569, 1622, 1627, 1924, 1934, 1941, 1978, 2234, 2301, 2527,\n",
       "        2528, 2532, 2612, 2654, 2769, 2784, 2826, 2840, 2887, 2942, 3120,\n",
       "        3217, 3314, 3328, 3338, 3361, 3410, 3412, 3472, 3596, 3899, 3980,\n",
       "        4037, 4117, 4123, 4155, 4247, 4254, 4278, 4384, 4428, 4651, 5041,\n",
       "        5263, 5336, 5372, 5468, 5495, 5637, 5653, 5661, 5682, 5712, 5767,\n",
       "        6063, 6106, 6117, 6172, 6256, 6350, 6463, 6576, 6580, 6601, 6740,\n",
       "        6809, 6971, 7098, 7158, 7182, 7207, 7260, 7344, 7374, 7377, 7402,\n",
       "        7404]),\n",
       " array([  80,  268,  325,  492,  730,  908,  986, 1152, 1157, 1217, 1251,\n",
       "        1258, 1282, 1386, 1450, 1593, 1658, 1677, 1688, 1746, 1762, 1822,\n",
       "        2049, 2153, 2177, 2183, 2190, 2246, 2392, 2399, 2458, 2468, 2504,\n",
       "        2595, 2667, 2723, 2882, 2903, 2950, 3048, 3063, 3073, 3359, 3447,\n",
       "        3536, 3604, 3623, 3665, 3746, 3856, 3864, 3930, 3949, 4237, 4329,\n",
       "        4435, 4495, 4644, 4662, 4670, 4692, 4774, 4785, 4877, 4887, 4953,\n",
       "        4971, 5048, 5050, 5524, 5549, 5579, 5676, 5755, 5864, 5932, 5954,\n",
       "        5959, 5962, 6262, 6298, 6319, 6328, 6339, 6447, 6593, 6602, 6761,\n",
       "        6896, 7018, 7038, 7202, 7234, 7297, 7336, 7338, 7381, 7411, 7422,\n",
       "        7441]),\n",
       " array([ 159,  319,  399,  440,  529,  533,  637,  660,  710,  751,  822,\n",
       "         895,  928,  936, 1237, 1438, 1494, 1584, 1605, 1635, 1651, 1653,\n",
       "        1663, 1736, 1844, 1878, 1992, 1994, 2120, 2162, 2189, 2192, 2205,\n",
       "        2212, 2278, 2541, 2593, 2597, 2622, 2696, 2817, 2837, 2893, 2983,\n",
       "        3023, 3173, 3188, 3386, 3452, 3499, 3528, 3654, 3688, 3703, 3798,\n",
       "        3948, 4031, 4157, 4267, 4290, 4306, 4341, 4354, 4361, 4380, 4508,\n",
       "        4529, 4543, 4727, 4853, 5017, 5039, 5147, 5190, 5247, 5318, 5326,\n",
       "        5391, 5528, 5558, 5575, 5581, 5638, 5838, 5892, 5971, 6083, 6257,\n",
       "        6388, 6472, 6609, 6690, 6807, 6822, 7099, 7162, 7164, 7170, 7257,\n",
       "        7301]),\n",
       " array([   6,  304,  546,  711,  729,  795,  901,  912, 1012, 1141, 1175,\n",
       "        1243, 1394, 1420, 1424, 1699, 1717, 1883, 1884, 1987, 2013, 2057,\n",
       "        2108, 2128, 2201, 2245, 2282, 2291, 2346, 2401, 2406, 2419, 2463,\n",
       "        2543, 2544, 2545, 2607, 2660, 2689, 2708, 2805, 3005, 3215, 3221,\n",
       "        3290, 3392, 3439, 3527, 3581, 3605, 3742, 3763, 3872, 3873, 3900,\n",
       "        4040, 4091, 4108, 4134, 4189, 4245, 4302, 4411, 4538, 4550, 4654,\n",
       "        4738, 4755, 4772, 4894, 4905, 5105, 5112, 5161, 5205, 5213, 5214,\n",
       "        5311, 5405, 5595, 5617, 5667, 5685, 5769, 5798, 5928, 5937, 5956,\n",
       "        6070, 6113, 6273, 6414, 6508, 6701, 6718, 7017, 7109, 7252, 7304,\n",
       "        7371]),\n",
       " array([ 183,  279,  365,  496,  670,  682,  721,  776,  926,  964, 1023,\n",
       "        1043, 1063, 1104, 1122, 1128, 1142, 1451, 1475, 1484, 1620, 1669,\n",
       "        1767, 1832, 1862, 1887, 1894, 1947, 1975, 2133, 2179, 2347, 2415,\n",
       "        2553, 2557, 2594, 2603, 2683, 2719, 2774, 2871, 2876, 2914, 2987,\n",
       "        3024, 3049, 3083, 3107, 3161, 3385, 3414, 3580, 3747, 3769, 3775,\n",
       "        3848, 4048, 4052, 4055, 4121, 4143, 4179, 4207, 4222, 4326, 4433,\n",
       "        4677, 4730, 4749, 4751, 4754, 4758, 4821, 4862, 4929, 4940, 5283,\n",
       "        5424, 5704, 5705, 5711, 5846, 5850, 5881, 6048, 6162, 6183, 6259,\n",
       "        6303, 6422, 6478, 6494, 6600, 6608, 6610, 6629, 6856, 6875, 6897,\n",
       "        7010]),\n",
       " array([  83,  174,  235,  338,  416,  549,  615,  653,  705,  752, 1016,\n",
       "        1067, 1079, 1200, 1240, 1304, 1347, 1372, 1378, 1432, 1447, 1466,\n",
       "        1472, 1524, 1538, 1567, 1661, 1803, 1805, 1867, 1948, 2031, 2037,\n",
       "        2063, 2069, 2164, 2220, 2281, 2356, 2363, 2381, 2397, 2590, 2712,\n",
       "        2718, 2782, 3114, 3347, 3355, 3379, 3415, 3443, 3575, 3690, 3725,\n",
       "        3937, 4075, 4288, 4352, 4355, 4416, 4418, 4457, 4488, 4494, 4531,\n",
       "        4607, 4684, 4724, 5120, 5162, 5201, 5225, 5287, 5573, 5620, 5760,\n",
       "        5777, 5778, 5803, 5890, 5922, 5987, 6074, 6146, 6175, 6187, 6205,\n",
       "        6435, 6471, 6491, 6683, 6699, 6735, 6747, 6829, 6955, 7025, 7255,\n",
       "        7332]),\n",
       " array([   7,  133,  224,  288,  331,  390,  606,  612,  645,  687,  692,\n",
       "         704,  782,  838,  910,  983, 1051, 1210, 1211, 1219, 1293, 1339,\n",
       "        1344, 1385, 1396, 1600, 1682, 1726, 2403, 2478, 2584, 2716, 2763,\n",
       "        2771, 2863, 2946, 2982, 3075, 3236, 3267, 3311, 3326, 3419, 3451,\n",
       "        3491, 3529, 3695, 3707, 3748, 3817, 3935, 3978, 4057, 4269, 4293,\n",
       "        4312, 4323, 4378, 4577, 4629, 4718, 4789, 4881, 5049, 5124, 5136,\n",
       "        5176, 5217, 5269, 5303, 5492, 5651, 5716, 5721, 5808, 5871, 5886,\n",
       "        5957, 6053, 6067, 6289, 6313, 6316, 6334, 6390, 6485, 6516, 6607,\n",
       "        6617, 6655, 6659, 6741, 6795, 6842, 6915, 7041, 7126, 7172, 7446,\n",
       "        7480]),\n",
       " array([  45,   84,  184,  330,  393,  404,  426,  430,  455,  507,  539,\n",
       "         589,  641,  672,  678,  971, 1030, 1033, 1047, 1065, 1087, 1114,\n",
       "        1207, 1356, 1357, 1380, 1582, 1681, 1716, 1800, 1904, 1981, 2029,\n",
       "        2040, 2089, 2259, 2284, 2288, 2329, 2374, 2497, 2515, 2639, 2643,\n",
       "        2839, 2921, 2954, 3000, 3123, 3156, 3202, 3436, 3532, 3569, 3616,\n",
       "        3685, 3861, 3892, 3919, 3925, 3934, 4019, 4113, 4128, 4195, 4277,\n",
       "        4403, 4447, 4526, 4621, 4643, 4657, 4815, 4818, 4979, 5062, 5260,\n",
       "        5332, 5357, 5564, 5801, 5807, 5855, 6034, 6041, 6087, 6092, 6118,\n",
       "        6295, 6375, 6520, 6615, 6748, 6857, 6870, 6872, 7013, 7037, 7047,\n",
       "        7224]),\n",
       " array([  11,  113,  442,  461,  521,  835,  925, 1074, 1089, 1131, 1209,\n",
       "        1265, 1276, 1328, 1377, 1427, 1511, 1539, 1565, 1602, 1623, 1625,\n",
       "        1706, 1787, 1917, 1982, 1997, 2106, 2219, 2232, 2377, 2447, 2506,\n",
       "        2549, 2690, 2787, 2810, 2828, 2995, 3115, 3118, 3268, 3297, 3330,\n",
       "        3380, 3396, 3469, 3749, 3812, 3882, 3943, 4018, 4038, 4119, 4145,\n",
       "        4297, 4553, 4610, 4613, 4729, 4739, 4813, 5007, 5096, 5243, 5265,\n",
       "        5268, 5377, 5462, 5553, 5593, 5606, 5690, 5707, 5811, 5909, 5915,\n",
       "        6101, 6160, 6282, 6321, 6383, 6410, 6415, 6449, 6634, 6662, 6684,\n",
       "        6710, 6767, 6796, 6867, 6916, 7012, 7154, 7200, 7211, 7280, 7314,\n",
       "        7454]),\n",
       " array([  58,   78,  165,  243,  335,  349,  432,  531,  596,  633,  658,\n",
       "         756,  860,  878,  909,  937, 1056, 1139, 1223, 1252, 1294, 1315,\n",
       "        1430, 1467, 1509, 1613, 1857, 1902, 2065, 2157, 2244, 2275, 2479,\n",
       "        2713, 2744, 2842, 2848, 2856, 2872, 2880, 2896, 2948, 2964, 2999,\n",
       "        3038, 3112, 3174, 3205, 3428, 3558, 3578, 3607, 3717, 3754, 3756,\n",
       "        3890, 3994, 4046, 4059, 4105, 4203, 4661, 4673, 4680, 4809, 4891,\n",
       "        4973, 5178, 5253, 5299, 5404, 5437, 5501, 5534, 5556, 5669, 5686,\n",
       "        5765, 5792, 5795, 5810, 5812, 5948, 5961, 6039, 6065, 6192, 6312,\n",
       "        6335, 6457, 6540, 6558, 6694, 6985, 7185, 7321, 7343, 7372, 7384,\n",
       "        7499]),\n",
       " array([  59,   70,  197,  453,  460,  603,  611,  613,  620,  655,  735,\n",
       "         741,  886,  970, 1135, 1374, 1457, 1519, 1615, 1749, 1776, 1799,\n",
       "        1951, 2070, 2097, 2137, 2146, 2290, 2385, 2448, 2452, 2471, 2496,\n",
       "        2507, 2555, 2591, 2610, 2659, 3092, 3158, 3282, 3289, 3292, 3306,\n",
       "        3437, 3554, 3599, 3609, 3735, 3938, 3998, 4043, 4173, 4328, 4366,\n",
       "        4373, 4392, 4406, 4421, 4426, 4440, 4649, 4678, 4944, 4974, 5191,\n",
       "        5241, 5278, 5290, 5308, 5541, 5642, 5717, 5983, 6026, 6103, 6214,\n",
       "        6217, 6227, 6275, 6291, 6369, 6440, 6531, 6536, 6559, 6578, 6589,\n",
       "        6642, 6771, 6806, 6821, 6852, 6909, 7086, 7228, 7238, 7248, 7358,\n",
       "        7487]),\n",
       " array([ 119,  142,  156,  164,  263,  343,  351,  378,  488,  591,  593,\n",
       "         642,  686,  765,  773,  880,  921,  943,  977, 1075, 1134, 1201,\n",
       "        1263, 1355, 1410, 1527, 1568, 1898, 1901, 2156, 2200, 2376, 2432,\n",
       "        2574, 2685, 2692, 2739, 2743, 2909, 2932, 3001, 3052, 3127, 3325,\n",
       "        3504, 3560, 3738, 4118, 4146, 4268, 4386, 4489, 4579, 4716, 4764,\n",
       "        4767, 4870, 4928, 4947, 5042, 5060, 5087, 5141, 5220, 5309, 5446,\n",
       "        5516, 5522, 5563, 5619, 5636, 5714, 5747, 5936, 6035, 6079, 6096,\n",
       "        6110, 6193, 6318, 6492, 6493, 6605, 6616, 6652, 6686, 6742, 6778,\n",
       "        6819, 6934, 6974, 7014, 7060, 7087, 7155, 7214, 7289, 7303, 7407,\n",
       "        7445]),\n",
       " array([  16,   17,   20,  121,  122,  127,  155,  439,  526,  543,  607,\n",
       "         717,  775,  837,  839, 1196, 1248, 1407, 1439, 1452, 1585, 1770,\n",
       "        1797, 1890, 1897, 2136, 2371, 2420, 2467, 2494, 2681, 2686, 2722,\n",
       "        2800, 2929, 2930, 3144, 3232, 3313, 3369, 3509, 3646, 3713, 3780,\n",
       "        3789, 3924, 3957, 3996, 4020, 4033, 4100, 4226, 4311, 4333, 4431,\n",
       "        4558, 4735, 4856, 4972, 4989, 5014, 5032, 5133, 5219, 5292, 5583,\n",
       "        5589, 5622, 5756, 5757, 5793, 5860, 5905, 5949, 5970, 6206, 6222,\n",
       "        6315, 6351, 6497, 6505, 6535, 6551, 6636, 6722, 6732, 7070, 7105,\n",
       "        7136, 7153, 7184, 7187, 7222, 7309, 7311, 7350, 7351, 7390, 7416,\n",
       "        7473]),\n",
       " array([  32,  128,  215,  278,  286,  588,  697,  738,  788,  820,  827,\n",
       "         851,  856, 1059, 1084, 1148, 1337, 1376, 1409, 1486, 1507, 1550,\n",
       "        1578, 1691, 1704, 1806, 1809, 1837, 2007, 2101, 2336, 2472, 2495,\n",
       "        2632, 2799, 2834, 2861, 2977, 3062, 3081, 3104, 3216, 3315, 3371,\n",
       "        3401, 3433, 3517, 3563, 3600, 3680, 3708, 3711, 3853, 3920, 4056,\n",
       "        4082, 4466, 4485, 4603, 4633, 4906, 5145, 5180, 5378, 5384, 5418,\n",
       "        5421, 5482, 5514, 5519, 5548, 5568, 5632, 5643, 5708, 5720, 5790,\n",
       "        6061, 6119, 6138, 6155, 6168, 6173, 6179, 6243, 6342, 6361, 6382,\n",
       "        6397, 6473, 6568, 6583, 6663, 6743, 6782, 6953, 7050, 7235, 7251,\n",
       "        7484]),\n",
       " array([ 150,  401,  462,  504,  564,  793,  823, 1046, 1150, 1232, 1306,\n",
       "        1433, 1557, 1560, 1614, 1741, 1928, 2173, 2267, 2338, 2359, 2379,\n",
       "        2416, 2444, 2572, 2649, 2650, 2666, 2738, 2807, 2959, 2979, 3009,\n",
       "        3013, 3015, 3065, 3088, 3108, 3353, 3449, 3506, 3551, 3553, 3650,\n",
       "        3701, 3821, 3843, 3870, 3881, 4094, 4102, 4169, 4184, 4219, 4240,\n",
       "        4246, 4336, 4430, 4491, 4584, 4628, 4686, 4697, 4712, 4984, 5027,\n",
       "        5063, 5134, 5146, 5186, 5297, 5305, 5322, 5438, 5594, 5657, 5736,\n",
       "        5779, 5869, 6011, 6055, 6095, 6177, 6180, 6238, 6358, 6669, 6779,\n",
       "        6799, 6816, 6949, 7045, 7291, 7295, 7317, 7364, 7436, 7439, 7460,\n",
       "        7464]),\n",
       " array([  19,   85,  176,  250,  258,  282,  315,  391,  412,  512,  514,\n",
       "         532,  537,  577,  671,  720,  834,  899, 1022, 1098, 1110, 1180,\n",
       "        1341, 1412, 1485, 1493, 1526, 1573, 1579, 1587, 1687, 1689, 1744,\n",
       "        2226, 2227, 2402, 2430, 2460, 2474, 2508, 2546, 2748, 2819, 2919,\n",
       "        2944, 3003, 3053, 3322, 3398, 3448, 3606, 3814, 3947, 3969, 3976,\n",
       "        4012, 4136, 4147, 4149, 4231, 4235, 4325, 4397, 4513, 4533, 4554,\n",
       "        4672, 4728, 4816, 4858, 5114, 5245, 5304, 5339, 5350, 5485, 5510,\n",
       "        5525, 5735, 5877, 6144, 6170, 6178, 6203, 6525, 6541, 6591, 6633,\n",
       "        6682, 6931, 7042, 7141, 7147, 7196, 7232, 7243, 7292, 7361, 7367,\n",
       "        7383]),\n",
       " array([  23,   31,  120,  146,  172,  202,  253,  299,  395,  405,  534,\n",
       "         706,  872,  875, 1014, 1101, 1225, 1443, 1478, 1545, 1552, 1843,\n",
       "        1915, 1959, 1980, 2112, 2118, 2134, 2335, 2360, 2364, 2367, 2539,\n",
       "        2565, 2688, 2709, 2730, 2846, 2910, 2949, 2975, 3002, 3010, 3047,\n",
       "        3103, 3137, 3258, 3323, 3358, 3360, 3406, 3459, 3628, 3684, 3729,\n",
       "        3757, 3773, 3887, 4096, 4551, 4766, 4831, 4878, 4909, 4980, 4997,\n",
       "        5065, 5126, 5127, 5131, 5143, 5181, 5195, 5289, 5330, 5493, 5734,\n",
       "        5753, 5773, 5806, 5814, 5822, 5947, 5995, 6075, 6190, 6218, 6226,\n",
       "        6300, 6324, 6330, 6714, 6783, 6865, 6948, 6988, 7152, 7174, 7270,\n",
       "        7288]),\n",
       " array([ 217,  281,  307,  328,  376,  585,  695,  701,  858, 1017, 1024,\n",
       "        1037, 1092, 1153, 1257, 1269, 1363, 1404, 1546, 1577, 1714, 1877,\n",
       "        1891, 1906, 2054, 2110, 2122, 2302, 2482, 2560, 2741, 2757, 2772,\n",
       "        2862, 2873, 2878, 3040, 3098, 3113, 3287, 3321, 3332, 3339, 3381,\n",
       "        3391, 3420, 3423, 3462, 3526, 3561, 3589, 3728, 3953, 4016, 4095,\n",
       "        4301, 4313, 4334, 4391, 4500, 4539, 4624, 4656, 4659, 4681, 4720,\n",
       "        4983, 5047, 5055, 5086, 5148, 5419, 5473, 5532, 5551, 5627, 5650,\n",
       "        5662, 5832, 5878, 5879, 5999, 6093, 6266, 6338, 6368, 6431, 6495,\n",
       "        6502, 6606, 6695, 6800, 6866, 6868, 6935, 7114, 7173, 7395, 7443,\n",
       "        7451]),\n",
       " array([ 219,  355,  366,  373,  550,  623,  673,  712,  739,  840,  873,\n",
       "         877,  889,  933, 1039, 1124, 1192, 1245, 1281, 1327, 1333, 1399,\n",
       "        1418, 1428, 1572, 1707, 1727, 1745, 1752, 1798, 2053, 2061, 2277,\n",
       "        2337, 2370, 2400, 2436, 2568, 2694, 2750, 2892, 2969, 2994, 3154,\n",
       "        3210, 3241, 3286, 3296, 3303, 3400, 3440, 3610, 3620, 3846, 3896,\n",
       "        3926, 3987, 4291, 4468, 4706, 4932, 5067, 5192, 5212, 5251, 5256,\n",
       "        5447, 5459, 5538, 5571, 5603, 5670, 5701, 5785, 5791, 5841, 6054,\n",
       "        6116, 6200, 6201, 6228, 6274, 6290, 6296, 6510, 6693, 6736, 6772,\n",
       "        6803, 6858, 6859, 6883, 6888, 6903, 7143, 7223, 7272, 7353, 7362,\n",
       "        7474]),\n",
       " array([ 111,  118,  262,  283,  327,  397,  417,  457,  506,  625,  639,\n",
       "         696,  754,  855,  882, 1072, 1238, 1244, 1571, 1606, 1910, 1960,\n",
       "        2055, 2066, 2073, 2090, 2130, 2138, 2160, 2222, 2223, 2327, 2518,\n",
       "        2583, 2728, 2754, 2765, 2867, 2881, 3055, 3105, 3152, 3235, 3270,\n",
       "        3362, 3416, 3435, 3505, 3550, 3573, 3582, 3671, 3726, 4007, 4168,\n",
       "        4213, 4238, 4399, 4410, 4422, 4477, 4534, 4761, 4780, 4889, 4988,\n",
       "        5031, 5070, 5168, 5511, 5560, 5602, 5615, 5820, 5868, 5997, 6044,\n",
       "        6045, 6165, 6212, 6242, 6285, 6325, 6333, 6374, 6386, 6469, 6730,\n",
       "        6773, 6958, 6993, 7001, 7007, 7019, 7160, 7249, 7266, 7320, 7365,\n",
       "        7403]),\n",
       " array([  50,   75,  109,  138,  216,  292,  356,  362,  444,  464,  554,\n",
       "         700,  868,  957,  995, 1155, 1187, 1365, 1421, 1473, 1499, 1621,\n",
       "        1783, 1804, 1893, 1964, 2038, 2169, 2470, 2492, 2510, 2542, 2664,\n",
       "        2734, 2745, 2802, 2885, 2905, 2937, 2968, 3059, 3094, 3121, 3149,\n",
       "        3257, 3288, 3334, 3372, 3466, 3570, 3630, 3697, 3733, 3829, 3854,\n",
       "        4049, 4196, 4253, 4305, 4537, 4578, 4671, 4746, 4835, 4855, 4917,\n",
       "        4970, 4975, 5104, 5254, 5439, 5486, 5545, 5569, 5580, 5683, 5742,\n",
       "        5746, 5749, 5764, 5835, 5921, 6174, 6185, 6232, 6327, 6344, 6448,\n",
       "        6459, 6490, 6564, 6738, 6907, 7071, 7138, 7213, 7348, 7423, 7424,\n",
       "        7453]),\n",
       " array([ 154,  239,  370,  423,  443,  502,  551,  584,  608,  669,  713,\n",
       "         746,  831,  945, 1080, 1081, 1109, 1172, 1212, 1312, 1351, 1354,\n",
       "        1479, 1547, 1562, 1684, 1721, 1734, 1793, 1810, 2016, 2135, 2211,\n",
       "        2273, 2306, 2418, 2505, 2519, 2531, 2635, 2700, 2789, 2821, 2833,\n",
       "        3078, 3117, 3190, 3291, 3399, 3409, 3486, 3495, 3512, 3519, 3565,\n",
       "        3594, 3800, 3876, 4003, 4011, 4058, 4138, 4215, 4234, 4276, 4459,\n",
       "        4478, 4566, 4689, 4690, 4719, 4802, 4834, 5051, 5100, 5526, 5547,\n",
       "        5633, 5687, 5688, 5698, 5902, 5939, 6089, 6213, 6329, 6511, 6706,\n",
       "        6709, 6723, 6848, 6913, 6933, 7131, 7167, 7244, 7245, 7373, 7375,\n",
       "        7386]),\n",
       " array([   9,   95,  145,  195,  227,  244,  271,  276,  321,  326,  344,\n",
       "         374,  454,  580,  586,  609,  662,  725,  790,  802,  808,  850,\n",
       "         975, 1070, 1102, 1203, 1288, 1362, 1638, 1645, 1869, 1886, 1958,\n",
       "        1963, 1979, 1996, 2050, 2064, 2171, 2253, 2548, 2571, 2630, 2888,\n",
       "        2912, 3030, 3277, 3389, 3730, 3813, 3850, 3952, 4103, 4193, 4206,\n",
       "        4275, 4281, 4318, 4414, 4439, 4467, 4483, 4510, 4573, 4747, 4748,\n",
       "        4851, 4907, 5030, 5045, 5097, 5121, 5266, 5340, 5376, 5433, 5675,\n",
       "        5718, 5796, 5883, 5885, 6069, 6080, 6417, 6552, 6802, 6810, 6811,\n",
       "        6901, 6902, 6963, 6983, 6990, 7137, 7181, 7324, 7327, 7409, 7467,\n",
       "        7488]),\n",
       " array([  41,   57,   98,  241,  381,  389,  474,  493,  505,  530,  815,\n",
       "         891, 1118, 1158, 1168, 1246, 1291, 1313, 1329, 1342, 1375, 1382,\n",
       "        1398, 1437, 1465, 1575, 1598, 1631, 1850, 2039, 2048, 2144, 2297,\n",
       "        2325, 2343, 2361, 2561, 2662, 2759, 2823, 2853, 2865, 3138, 3265,\n",
       "        3281, 3357, 3407, 3465, 3679, 3791, 3874, 3895, 3902, 4122, 4289,\n",
       "        4321, 4393, 4437, 4581, 4691, 4741, 4833, 4867, 4897, 4901, 4913,\n",
       "        4966, 4994, 5102, 5165, 5231, 5258, 5481, 5503, 5587, 5611, 5766,\n",
       "        5862, 5887, 5917, 6081, 6346, 6352, 6362, 6365, 6373, 6412, 6873,\n",
       "        6882, 6912, 6942, 6944, 6960, 6965, 7068, 7177, 7240, 7363, 7463,\n",
       "        7469]),\n",
       " array([  27,  196,  234,  316,  318,  648,  807,  939,  991, 1026, 1126,\n",
       "        1239, 1241, 1284, 1302, 1364, 1514, 1637, 1759, 1764, 1784, 1889,\n",
       "        1962, 2015, 2035, 2176, 2238, 2268, 2299, 2396, 2429, 2490, 2581,\n",
       "        2634, 2658, 2818, 2824, 2957, 2965, 2991, 3044, 3061, 3077, 3124,\n",
       "        3136, 3199, 3404, 3482, 3501, 3577, 3587, 3602, 3714, 3745, 3820,\n",
       "        3940, 3999, 4109, 4115, 4129, 4132, 4150, 4223, 4233, 4465, 4559,\n",
       "        4609, 4641, 4647, 4820, 4945, 4954, 5004, 5022, 5059, 5152, 5153,\n",
       "        5242, 5371, 5520, 5833, 5899, 5935, 5950, 5967, 6002, 6008, 6245,\n",
       "        6281, 6283, 6792, 6947, 7035, 7096, 7100, 7159, 7191, 7233, 7273,\n",
       "        7394]),\n",
       " array([  21,   69,   77,  126,  190,  238,  295,  361,  449,  627,  854,\n",
       "         866,  893, 1057, 1094, 1292, 1324, 1367, 1489, 1490, 1515, 1516,\n",
       "        1529, 1558, 1713, 1814, 1918, 1954, 1983, 2021, 2152, 2251, 2462,\n",
       "        2608, 2613, 2655, 2803, 3006, 3132, 3272, 3273, 3408, 3424, 3444,\n",
       "        3457, 3470, 3484, 3621, 3638, 3663, 3705, 3866, 3909, 3966, 3974,\n",
       "        4104, 4255, 4260, 4324, 4365, 4612, 4769, 4823, 4837, 4846, 4874,\n",
       "        4880, 4882, 4958, 5052, 5151, 5218, 5223, 5435, 5469, 5566, 5702,\n",
       "        5729, 6082, 6100, 6189, 6225, 6343, 6533, 6546, 6573, 6658, 6660,\n",
       "        6679, 6700, 6777, 6780, 6869, 6989, 7076, 7101, 7236, 7356, 7450,\n",
       "        7490]),\n",
       " array([   2,  101,  171,  275,  358,  561,  810,  861,  935,  947,  996,\n",
       "        1008, 1018, 1050, 1088, 1256, 1411, 1442, 1445, 1536, 1815, 1848,\n",
       "        1864, 1945, 1969, 2047, 2081, 2107, 2242, 2357, 2383, 2409, 2417,\n",
       "        2425, 2480, 2525, 2605, 2625, 2697, 2899, 2920, 2947, 3106, 3167,\n",
       "        3340, 3344, 3492, 3494, 3597, 3629, 3664, 3668, 3720, 3751, 3838,\n",
       "        3911, 4035, 4141, 4214, 4271, 4331, 4398, 4438, 4498, 4593, 4617,\n",
       "        4726, 4768, 4924, 5185, 5196, 5295, 5346, 5366, 5489, 5527, 5614,\n",
       "        5727, 5759, 5870, 5918, 6122, 6406, 6499, 6554, 6648, 6675, 6753,\n",
       "        6759, 6804, 6846, 6861, 6904, 7027, 7103, 7229, 7278, 7322, 7398,\n",
       "        7447]),\n",
       " array([ 187,  191,  200,  436,  450,  484,  562,  568,  664,  786,  844,\n",
       "         896, 1066, 1322, 1332, 1429, 1522, 1624, 1722, 1772, 1926, 2077,\n",
       "        2080, 2092, 2150, 2170, 2313, 2398, 2421, 2500, 2633, 2804, 2879,\n",
       "        3099, 3134, 3153, 3259, 3453, 3488, 3586, 3651, 3682, 3871, 3918,\n",
       "        3956, 3977, 4027, 4110, 4131, 4175, 4212, 4224, 4239, 4357, 4381,\n",
       "        4450, 4451, 4454, 4497, 4499, 4524, 4530, 4555, 4715, 4827, 5034,\n",
       "        5037, 5099, 5288, 5365, 5390, 5480, 5512, 5521, 5535, 5585, 5604,\n",
       "        5618, 5640, 5646, 5668, 5678, 5774, 6102, 6221, 6249, 6455, 6467,\n",
       "        6685, 6707, 6788, 6797, 6826, 6831, 6970, 7107, 7345, 7357, 7393,\n",
       "        7426]),\n",
       " array([  94,  115,  226,  353,  386,  481,  509,  558,  610,  743,  853,\n",
       "         883, 1041, 1106, 1137, 1154, 1189, 1345, 1453, 1646, 1720, 1876,\n",
       "        1912, 1946, 1956, 2036, 2042, 2208, 2224, 2247, 2250, 2464, 2489,\n",
       "        2491, 2503, 2521, 2579, 2644, 2671, 2699, 2732, 2752, 2904, 2924,\n",
       "        2962, 3012, 3143, 3181, 3237, 3278, 3497, 3523, 3568, 3659, 3669,\n",
       "        3761, 3781, 3815, 3860, 3897, 3959, 4044, 4273, 4453, 4463, 4474,\n",
       "        4493, 4606, 4674, 4682, 4848, 4859, 4876, 4886, 4900, 5095, 5139,\n",
       "        5331, 5375, 5380, 5382, 5410, 5518, 5561, 5664, 6003, 6050, 6078,\n",
       "        6403, 6413, 6566, 6586, 6643, 6674, 6784, 6853, 7067, 7095, 7129,\n",
       "        7151]),\n",
       " array([  81,  168,  221,  240,  293,  334,  429,  569,  630,  698,  726,\n",
       "         771,  929,  985, 1027, 1032, 1138, 1190, 1308, 1320, 1403, 1460,\n",
       "        1512, 1532, 1603, 1648, 1662, 1773, 1788, 1866, 2012, 2149, 2185,\n",
       "        2217, 2235, 2332, 2526, 2547, 2550, 2592, 2735, 2747, 2767, 3011,\n",
       "        3223, 3231, 3266, 3374, 3395, 3525, 3590, 3622, 3642, 3797, 3839,\n",
       "        3847, 3898, 3968, 4008, 4063, 4135, 4292, 4623, 4666, 4688, 4842,\n",
       "        4936, 4960, 5024, 5040, 5066, 5074, 5227, 5250, 5356, 5429, 5693,\n",
       "        5739, 5780, 5847, 5897, 5980, 6004, 6108, 6111, 6124, 6548, 6638,\n",
       "        6664, 6737, 6841, 6850, 6945, 6961, 7029, 7157, 7197, 7217, 7259,\n",
       "        7354]),\n",
       " array([  30,   42,  225,  257,  332,  491,  780,  876,  990, 1053, 1090,\n",
       "        1198, 1392, 1549, 1636, 1666, 1838, 1854, 1907, 1919, 1950, 2175,\n",
       "        2204, 2216, 2230, 2239, 2344, 2368, 2449, 2587, 2626, 2687, 2701,\n",
       "        2822, 2852, 2869, 2967, 3029, 3220, 3704, 3858, 3867, 3942, 3945,\n",
       "        3962, 4028, 4045, 4274, 4351, 4413, 4442, 4505, 4597, 4622, 4625,\n",
       "        4843, 4865, 4941, 5128, 5272, 5369, 5370, 5423, 5463, 5607, 5709,\n",
       "        5750, 5761, 5788, 5817, 6043, 6136, 6149, 6158, 6253, 6370, 6404,\n",
       "        6420, 6512, 6537, 6539, 6594, 6627, 6801, 6825, 6864, 6874, 6954,\n",
       "        6956, 7003, 7023, 7044, 7056, 7065, 7168, 7267, 7330, 7428, 7459,\n",
       "        7475]),\n",
       " array([  44,   99,  163,  201,  232,  233,  341,  385,  458,  583,  602,\n",
       "         628,  777,  879,  953,  998, 1000, 1167, 1259, 1318, 1395, 1423,\n",
       "        1480, 1879, 1977, 1991, 2076, 2139, 2174, 2214, 2265, 2308, 2354,\n",
       "        2428, 2434, 2537, 2669, 2764, 3280, 3341, 3343, 3441, 3446, 3643,\n",
       "        3667, 3678, 3718, 3766, 3778, 3804, 3869, 3879, 3963, 4002, 4081,\n",
       "        4099, 4205, 4236, 4243, 4360, 4460, 4571, 4576, 4605, 4899, 4978,\n",
       "        4986, 5021, 5351, 5460, 5467, 5554, 5654, 5703, 5731, 5866, 5872,\n",
       "        6029, 6068, 6277, 6341, 6345, 6393, 6405, 6419, 6451, 6481, 6526,\n",
       "        6571, 6584, 6793, 6794, 6820, 6839, 6884, 7000, 7063, 7097, 7310,\n",
       "        7497]),\n",
       " array([  46,   60,   72,   79,  186,  209,  337,  394,  445,  476,  814,\n",
       "         842,  898,  944,  984, 1035, 1279, 1335, 1387, 1436, 1469, 1505,\n",
       "        1570, 1632, 1825, 1896, 2000, 2028, 2178, 2294, 2311, 2352, 2435,\n",
       "        2559, 2646, 2705, 2952, 2953, 2980, 3069, 3165, 3184, 3225, 3368,\n",
       "        3378, 3387, 3533, 3546, 3595, 3618, 3884, 3914, 3928, 4053, 4176,\n",
       "        4242, 4257, 4338, 4402, 4585, 4722, 4784, 4788, 4836, 4838, 4948,\n",
       "        4963, 5010, 5012, 5023, 5080, 5119, 5208, 5325, 5416, 5559, 5889,\n",
       "        5972, 6148, 6286, 6302, 6425, 6430, 6439, 6450, 6479, 6488, 6496,\n",
       "        6501, 6668, 6876, 6936, 6959, 7009, 7046, 7094, 7142, 7263, 7308,\n",
       "        7442]),\n",
       " array([  54,  188,  251,  264,  324,  368,  482,  659,  665,  724,  794,\n",
       "         902,  919,  981,  994, 1028, 1052, 1103, 1230, 1340, 1419, 1517,\n",
       "        1548, 1611, 1633, 1654, 1659, 1710, 1756, 1777, 1870, 1940, 2196,\n",
       "        2228, 2249, 2289, 2348, 2438, 2604, 2641, 2783, 3017, 3100, 3140,\n",
       "        3187, 3251, 3260, 3269, 3403, 3534, 3672, 3779, 3885, 3893, 3931,\n",
       "        3939, 4001, 4120, 4190, 4251, 4295, 4353, 4387, 4394, 4420, 4583,\n",
       "        4663, 4779, 4824, 4961, 5088, 5216, 5228, 5472, 5715, 5748, 5751,\n",
       "        5821, 5943, 6049, 6051, 6129, 6137, 6166, 6171, 6379, 6396, 6596,\n",
       "        6620, 6667, 6713, 6891, 6926, 6995, 7064, 7116, 7203, 7406, 7434,\n",
       "        7444]),\n",
       " array([   3,   29,   91,  270,  310,  424,  427,  566,  626,  728,  762,\n",
       "         809,  867,  870,  894, 1117, 1218, 1384, 1441, 1534, 1650, 1656,\n",
       "        1672, 1695, 1748, 1785, 1913, 1993, 2062, 2085, 2154, 2293, 2296,\n",
       "        2569, 2614, 2891, 2940, 2984, 3186, 3194, 3474, 3507, 3543, 3548,\n",
       "        3559, 3576, 3603, 3845, 4004, 4006, 4013, 4062, 4071, 4089, 4092,\n",
       "        4265, 4322, 4452, 4567, 4653, 4658, 4869, 4902, 4985, 5026, 5053,\n",
       "        5071, 5169, 5232, 5255, 5257, 5291, 5337, 5342, 5367, 5576, 5752,\n",
       "        5786, 5794, 5856, 5938, 5988, 6077, 6195, 6210, 6381, 6454, 6465,\n",
       "        6569, 6582, 6590, 6680, 6751, 6860, 6925, 7032, 7034, 7124, 7307,\n",
       "        7470]),\n",
       " array([  35,  189,  252,  265,  314,  340,  433,  542,  667,  684,  999,\n",
       "        1009, 1021, 1121, 1125, 1184, 1286, 1360, 1393, 1444, 1454, 1491,\n",
       "        1576, 1641, 1781, 1791, 1920, 2003, 2008, 2369, 2410, 2412, 2427,\n",
       "        2466, 2627, 2631, 2727, 2760, 2777, 2836, 2926, 3276, 3520, 3686,\n",
       "        3716, 3782, 4005, 4017, 4070, 4101, 4272, 4342, 4343, 4374, 4655,\n",
       "        4733, 4742, 4808, 4969, 5038, 5149, 5166, 5167, 5234, 5271, 5293,\n",
       "        5310, 5333, 5417, 5451, 5452, 5546, 5565, 5677, 5724, 5839, 5880,\n",
       "        5933, 5951, 5968, 5986, 5998, 6032, 6230, 6309, 6322, 6380, 6418,\n",
       "        6776, 6828, 6881, 7117, 7125, 7133, 7216, 7239, 7250, 7328, 7410,\n",
       "        7476]),\n",
       " array([   0,   25,  112,  153,  173,  205,  333,  437,  545,  548,  556,\n",
       "         619,  635,  638,  913, 1144, 1169, 1235, 1768, 1856, 2058, 2086,\n",
       "        2163, 2264, 2283, 2566, 2663, 2682, 2791, 2858, 2902, 2925, 2985,\n",
       "        3411, 3770, 3772, 3811, 3903, 3984, 4030, 4107, 4191, 4221, 4250,\n",
       "        4263, 4395, 4458, 4503, 4563, 4570, 4608, 4792, 4805, 4861, 4915,\n",
       "        5003, 5057, 5064, 5138, 5144, 5163, 5198, 5207, 5301, 5347, 5434,\n",
       "        5442, 5466, 5478, 5513, 5694, 5903, 5914, 5944, 5990, 5993, 6018,\n",
       "        6052, 6104, 6107, 6197, 6244, 6280, 6337, 6461, 6666, 6698, 6749,\n",
       "        6755, 6766, 6906, 6998, 7073, 7110, 7194, 7282, 7284, 7349, 7414,\n",
       "        7415]),\n",
       " array([  24,   39,  108,  261,  280,  552,  652,  663,  677,  681,  932,\n",
       "        1085, 1133, 1289, 1295, 1667, 1820, 1925, 2071, 2262, 2362, 2459,\n",
       "        2517, 2636, 2720, 2725, 2793, 2831, 2866, 2877, 2970, 2971, 2972,\n",
       "        2973, 2986, 3036, 3042, 3046, 3133, 3285, 3304, 3354, 3511, 3522,\n",
       "        3556, 3601, 3613, 3666, 3764, 3877, 3910, 3965, 4024, 4065, 4069,\n",
       "        4170, 4180, 4186, 4187, 4211, 4229, 4259, 4371, 4404, 4544, 4793,\n",
       "        4840, 4847, 4918, 4990, 5058, 5211, 5343, 5414, 5427, 5599, 5854,\n",
       "        5960, 6031, 6060, 6181, 6271, 6306, 6378, 6476, 6542, 6563, 6622,\n",
       "        6687, 6833, 6910, 6930, 7022, 7130, 7145, 7192, 7198, 7231, 7312,\n",
       "        7326]),\n",
       " array([  47,  208,  222,  431,  473,  617,  640,  718,  727,  733,  803,\n",
       "         811,  922,  949,  958, 1068, 1076, 1278, 1528, 1537, 1828, 1984,\n",
       "        1986, 1988, 2121, 2127, 2143, 2408, 2443, 3072, 3096, 3147, 3222,\n",
       "        3240, 3324, 3370, 3571, 3625, 3633, 3648, 3652, 3676, 3731, 3777,\n",
       "        3784, 3788, 3802, 3809, 4022, 4041, 4087, 4140, 4208, 4282, 4319,\n",
       "        4379, 4441, 4446, 4518, 4525, 4685, 4753, 4790, 4875, 4893, 4914,\n",
       "        4942, 5156, 5237, 5306, 5313, 5319, 5582, 5588, 5608, 5641, 5672,\n",
       "        5710, 5824, 5884, 5978, 6020, 6073, 6198, 6421, 6428, 6504, 6572,\n",
       "        6651, 6673, 6708, 6918, 6920, 7084, 7111, 7195, 7205, 7277, 7337,\n",
       "        7388]),\n",
       " array([  37,  508,  563,  575,  643,  674,  750,  778,  833,  903,  940,\n",
       "         979, 1111, 1119, 1182, 1343, 1401, 1477, 1483, 1542, 1551, 1643,\n",
       "        1760, 1782, 1909, 2026, 2272, 2349, 2493, 2499, 2600, 2619, 2704,\n",
       "        2768, 2797, 2857, 2935, 3018, 3041, 3157, 3169, 3262, 3336, 3375,\n",
       "        3418, 3632, 3762, 3793, 3807, 3819, 3822, 3836, 3912, 4068, 4112,\n",
       "        4162, 4228, 4256, 4286, 4486, 4496, 4627, 4773, 4832, 4998, 5132,\n",
       "        5233, 5294, 5358, 5395, 5396, 5422, 5456, 5830, 5896, 5963, 6015,\n",
       "        6088, 6268, 6367, 6453, 6567, 6587, 6769, 6786, 6862, 6899, 6905,\n",
       "        6973, 7048, 7051, 7090, 7106, 7183, 7294, 7300, 7323, 7346, 7347,\n",
       "        7401]),\n",
       " array([  38,   62,  131,  203,  256,  301,  342,  410,  465,  501,  649,\n",
       "         694,  749,  757,  923,  997, 1011, 1151, 1161, 1163, 1179, 1264,\n",
       "        1388, 1405, 1468, 1610, 1723, 1724, 1771, 1796, 1855, 1942, 2027,\n",
       "        2113, 2198, 2256, 2295, 2358, 2433, 2456, 2520, 2530, 2601, 2868,\n",
       "        2894, 3043, 3060, 3228, 3427, 3461, 3498, 3574, 3588, 3694, 3933,\n",
       "        3991, 4025, 4047, 4051, 4116, 4167, 4192, 4199, 4225, 4248, 4423,\n",
       "        4455, 4602, 4619, 4760, 4806, 4879, 4896, 5187, 5373, 5626, 5665,\n",
       "        5684, 5829, 5919, 5923, 5929, 6040, 6059, 6139, 6356, 6364, 6500,\n",
       "        6519, 6705, 6765, 6851, 6924, 6929, 7132, 7139, 7150, 7189, 7254,\n",
       "        7397]),\n",
       " array([  33,  134,  178,  223,  466,  714,  797,  836,  911,  960, 1055,\n",
       "        1062, 1145, 1149, 1160, 1171, 1188, 1213, 1448, 1470, 1609, 1709,\n",
       "        1827, 1875, 1900, 2011, 2326, 2512, 2529, 2551, 2652, 2737, 2749,\n",
       "        2781, 2889, 2989, 3076, 3079, 3130, 3142, 3155, 3219, 3352, 3537,\n",
       "        3538, 3640, 3808, 3831, 3915, 3944, 3958, 3988, 4098, 4185, 4298,\n",
       "        4300, 4317, 4396, 4405, 4556, 4630, 4660, 4668, 4783, 4845, 5158,\n",
       "        5202, 5229, 5264, 5341, 5353, 5577, 5649, 5882, 5927, 5991, 6109,\n",
       "        6150, 6199, 6220, 6248, 6347, 6432, 6592, 6612, 6661, 6739, 6746,\n",
       "        6768, 6798, 6894, 7061, 7082, 7085, 7119, 7247, 7253, 7293, 7355,\n",
       "        7421]),\n",
       " array([  40,   82,  123,  193,  231,  267,  479,  510,  572,  656,  832,\n",
       "         885,  974, 1006, 1061, 1093, 1156, 1208, 1250, 1358, 1495, 1513,\n",
       "        1665, 1686, 1858, 1871, 1873, 1905, 2006, 2213, 2243, 2451, 2536,\n",
       "        2653, 3019, 3026, 3074, 3135, 3234, 3294, 3342, 3515, 3634, 3645,\n",
       "        3660, 3693, 3776, 3922, 3982, 4086, 4130, 4210, 4232, 4330, 4471,\n",
       "        4479, 4511, 4547, 4604, 4786, 4850, 4931, 4976, 5001, 5159, 5307,\n",
       "        5383, 5542, 5555, 5659, 5740, 5758, 5828, 5837, 5845, 5906, 6076,\n",
       "        6126, 6182, 6186, 6251, 6292, 6320, 6385, 6409, 6429, 6549, 6781,\n",
       "        6922, 6979, 6986, 7113, 7144, 7156, 7246, 7283, 7305, 7380, 7382,\n",
       "        7483]),\n",
       " array([  96,  114,  290,  297,  409,  477,  604,  703,  737,  767,  783,\n",
       "         799,  804,  859,  938,  967, 1099, 1165, 1231, 1236, 1440, 1607,\n",
       "        1664, 1729, 1865, 1911, 1933, 2067, 2125, 2129, 2188, 2287, 2309,\n",
       "        2315, 2323, 2355, 2431, 2439, 2481, 2598, 2674, 2707, 2766, 2955,\n",
       "        3033, 3080, 3183, 3252, 3283, 3584, 3611, 3624, 3627, 3702, 3737,\n",
       "        3741, 3743, 3868, 4076, 4085, 4137, 4152, 4177, 4204, 4307, 4309,\n",
       "        4349, 4400, 4401, 4620, 4849, 4854, 4919, 4938, 4955, 5044, 5115,\n",
       "        5135, 5246, 5277, 5345, 5388, 5474, 5499, 5609, 5745, 5775, 5805,\n",
       "        5965, 6005, 6252, 6260, 6326, 6332, 6477, 6498, 6597, 6885, 7468,\n",
       "        7486]),\n",
       " array([  13,  162,  312,  380,  396,  624,  657, 1116, 1127, 1147, 1205,\n",
       "        1206, 1334, 1346, 1583, 1608, 1680, 1927, 1971, 2017, 2116, 2225,\n",
       "        2269, 2314, 2373, 2391, 2395, 2684, 2698, 2729, 2731, 2908, 2939,\n",
       "        3020, 3126, 3141, 3249, 3402, 3691, 3719, 3758, 3927, 3981, 4188,\n",
       "        4299, 4432, 4512, 4532, 4557, 4561, 4596, 4616, 4631, 4723, 4732,\n",
       "        4943, 5035, 5077, 5078, 5084, 5107, 5113, 5328, 5338, 5431, 5645,\n",
       "        5647, 5655, 5679, 5692, 5763, 5834, 5911, 5916, 6056, 6057, 6134,\n",
       "        6188, 6209, 6307, 6377, 6547, 6717, 6726, 6752, 6762, 6805, 6843,\n",
       "        6917, 6923, 6943, 6975, 7036, 7118, 7135, 7161, 7179, 7188, 7405,\n",
       "        7449]),\n",
       " array([  61,  135,  211,  218,  242,  303,  403,  544,  685,  759,  791,\n",
       "         849,  881,  966, 1136, 1226, 1234, 1296, 1353, 1400, 1464, 1540,\n",
       "        1644, 1939, 2082, 2155, 2158, 2194, 2229, 2300, 2333, 2405, 2475,\n",
       "        2516, 2623, 2661, 2756, 2906, 3008, 3027, 3087, 3271, 3275, 3376,\n",
       "        3383, 3467, 3510, 3635, 3681, 3706, 3709, 3715, 3792, 3832, 3889,\n",
       "        3908, 3986, 4074, 4097, 4126, 4249, 4320, 4376, 4448, 4519, 4521,\n",
       "        4574, 4701, 5028, 5123, 5215, 5284, 5362, 5389, 5394, 5428, 5557,\n",
       "        5628, 5699, 5783, 5789, 5823, 5826, 6033, 6130, 6153, 6207, 6236,\n",
       "        6443, 6460, 6595, 6703, 6757, 6814, 6932, 7080, 7091, 7275, 7313,\n",
       "        7482]),\n",
       " array([ 129,  320,  487,  644,  647,  847, 1001, 1038, 1086, 1112, 1170,\n",
       "        1222, 1325, 1336, 1359, 1435, 1456, 1601, 1674, 1761, 1812, 1846,\n",
       "        1874, 2074, 2084, 2317, 2498, 2535, 2609, 2670, 2676, 2679, 2761,\n",
       "        2916, 3067, 3179, 3247, 3316, 3350, 3471, 3490, 3500, 3513, 3530,\n",
       "        3631, 3655, 3752, 3790, 3796, 3827, 3888, 3992, 4021, 4111, 4310,\n",
       "        4363, 4372, 4417, 4586, 4664, 4734, 4771, 4817, 4819, 4987, 5016,\n",
       "        5081, 5334, 5411, 5441, 5634, 5671, 5776, 6097, 6355, 6359, 6389,\n",
       "        6446, 6474, 6484, 6545, 6550, 6621, 6646, 6725, 6789, 6838, 6940,\n",
       "        6982, 6997, 7030, 7062, 7081, 7208, 7242, 7352, 7378, 7389, 7481,\n",
       "        7498]),\n",
       " array([ 277,  345,  348,  350,  357,  422,  470,  650,  691,  715,  789,\n",
       "         798,  826,  888, 1045, 1120, 1146, 1159, 1679, 1849, 1860, 1961,\n",
       "        2046, 2181, 2351, 2411, 2423, 2440, 2477, 2534, 2578, 2628, 2794,\n",
       "        2966, 2998, 3032, 3093, 3180, 3227, 3349, 3489, 3503, 3653, 3834,\n",
       "        3961, 4026, 4088, 4090, 4127, 4266, 4270, 4287, 4618, 4640, 4778,\n",
       "        4796, 4864, 4908, 4921, 4999, 5092, 5118, 5275, 5344, 5406, 5430,\n",
       "        5470, 5624, 5725, 5770, 5771, 5809, 5825, 5888, 5925, 5931, 6007,\n",
       "        6009, 6036, 6091, 6114, 6115, 6157, 6163, 6176, 6204, 6239, 6263,\n",
       "        6264, 6314, 6506, 6518, 6570, 6637, 7112, 7193, 7325, 7452, 7462,\n",
       "        7472]),\n",
       " array([ 161,  485,  557,  675,  676,  689,  734,  792,  805,  830,  864,\n",
       "         968, 1044, 1073, 1083, 1166, 1185, 1461, 1657, 1698, 1775, 1842,\n",
       "        1968, 1976, 1995, 2132, 2165, 2339, 2345, 2350, 2378, 2394, 2413,\n",
       "        2450, 2513, 2552, 2657, 2790, 2928, 2933, 3084, 3293, 3377, 3485,\n",
       "        3555, 3585, 3614, 3736, 3970, 4178, 4194, 4346, 4347, 4359, 4523,\n",
       "        4541, 4714, 4777, 4937, 4951, 5085, 5090, 5206, 5224, 5281, 5374,\n",
       "        5381, 5415, 5457, 5487, 5550, 5552, 5586, 5744, 5840, 5843, 5941,\n",
       "        6038, 6112, 6143, 6246, 6311, 6444, 6640, 6644, 6653, 6921, 6951,\n",
       "        6991, 7021, 7031, 7040, 7079, 7171, 7199, 7215, 7227, 7281, 7418,\n",
       "        7478]),\n",
       " array([ 110,  346,  494,  555,  774,  821,  915,  930,  959, 1025, 1228,\n",
       "        1233, 1369, 1431, 1595, 1701, 1841, 1989, 1990, 2009, 2023, 2140,\n",
       "        2147, 2316, 2618, 2997, 3034, 3050, 3068, 3085, 3195, 3218, 3226,\n",
       "        3299, 3456, 3463, 3540, 3598, 3608, 3615, 3617, 3641, 3661, 3767,\n",
       "        3774, 3833, 3851, 4079, 4156, 4327, 4444, 4445, 4464, 4587, 4599,\n",
       "        4731, 4740, 4798, 4826, 4829, 4895, 4910, 4911, 4939, 5173, 5199,\n",
       "        5203, 5320, 5464, 5574, 5630, 5762, 5849, 5969, 6042, 6120, 6135,\n",
       "        6293, 6317, 6360, 6401, 6408, 6468, 6517, 6689, 6744, 6879, 6967,\n",
       "        6996, 7011, 7039, 7102, 7176, 7262, 7298, 7315, 7370, 7376, 7379,\n",
       "        7417]),\n",
       " array([   4,   64,  124,  291,  372,  419,  469,  500,  597,  723,  824,\n",
       "         871,  987, 1054, 1058, 1283, 1317, 1397, 1506, 1533, 1630, 1718,\n",
       "        1769, 1794, 1801, 1808, 1816, 1943, 1957, 1970, 2083, 2148, 2197,\n",
       "        2279, 2321, 2328, 2445, 2556, 2564, 2585, 2640, 2677, 2884, 2922,\n",
       "        2931, 3168, 3337, 3351, 3364, 3388, 3426, 3698, 3700, 3786, 3830,\n",
       "        3841, 3917, 3923, 4151, 4220, 4356, 4502, 4536, 4750, 4776, 4884,\n",
       "        4922, 4927, 5068, 5175, 5209, 5321, 5354, 5359, 5445, 5500, 5505,\n",
       "        5507, 5567, 5697, 6066, 6194, 6231, 6483, 6588, 6631, 6670, 6692,\n",
       "        6790, 6834, 6855, 6952, 7008, 7055, 7089, 7210, 7219, 7256, 7429,\n",
       "        7492]),\n",
       " array([  26,   66,   88,   90,  144,  354,  369,  503,  753,  787,  906,\n",
       "         924,  951, 1034, 1064, 1130, 1268, 1305, 1307, 1458, 1591, 1619,\n",
       "        1639, 1660, 1692, 1696, 1740, 1892, 2096, 2115, 2168, 2182, 2184,\n",
       "        2233, 2263, 2353, 2461, 2621, 2703, 2788, 2847, 2918, 2990, 3031,\n",
       "        3176, 3185, 3192, 3193, 3224, 3295, 3329, 3390, 3394, 3421, 3476,\n",
       "        3481, 3783, 3823, 3826, 3852, 3904, 3929, 3951, 3990, 4060, 4209,\n",
       "        4337, 4415, 4504, 4522, 4546, 4634, 4669, 4703, 4912, 4935, 4957,\n",
       "        4967, 4977, 5179, 5210, 5368, 5387, 5465, 5476, 5625, 5680, 5799,\n",
       "        5982, 6237, 6532, 6704, 6729, 6887, 6928, 6994, 7241, 7302, 7333,\n",
       "        7438]),\n",
       " array([   8,   68,  182,  245,  285,  329,  379,  414,  435,  605,  760,\n",
       "         857,  934,  948,  973,  992, 1173, 1310, 1426, 1521, 1555, 1592,\n",
       "        1599, 1647, 1712, 1731, 1823, 1852, 1863, 1936, 1998, 2079, 2142,\n",
       "        2193, 2389, 2446, 2473, 2673, 2758, 2845, 2923, 2958, 2981, 3004,\n",
       "        3051, 3102, 3196, 3255, 3348, 3455, 3544, 3673, 3710, 3739, 3787,\n",
       "        3803, 3878, 3960, 4080, 4142, 4161, 4261, 4285, 4407, 4449, 4552,\n",
       "        4694, 4708, 4709, 5029, 5402, 5454, 5498, 5539, 5681, 5754, 5893,\n",
       "        5920, 5996, 6223, 6229, 6276, 6434, 6544, 6618, 6657, 6754, 6824,\n",
       "        6871, 6966, 6978, 6987, 7005, 7271, 7286, 7334, 7385, 7412, 7458,\n",
       "        7471]),\n",
       " array([ 104,  116,  139,  367,  421,  497,  565,  573,  582,  690,  904,\n",
       "         941,  972, 1216, 1261, 1274, 1361, 1370, 1580, 1588, 1675, 1702,\n",
       "        1757, 1758, 1763, 1790, 1908, 1921, 2022, 2059, 2087, 2390, 2562,\n",
       "        2795, 2895, 2960, 3016, 3028, 3035, 3254, 3516, 3579, 3647, 3689,\n",
       "        3712, 3771, 3835, 3880, 3894, 3985, 4000, 4172, 4197, 4304, 4382,\n",
       "        4388, 4469, 4569, 4588, 4632, 4667, 4704, 4711, 4765, 4781, 4814,\n",
       "        5000, 5020, 5046, 5111, 5197, 5230, 5235, 5236, 5239, 5261, 5400,\n",
       "        5448, 5449, 5529, 5616, 5836, 5953, 5979, 6001, 6012, 6156, 6267,\n",
       "        6272, 6437, 6462, 6565, 6625, 6628, 6716, 6941, 7108, 7134, 7148,\n",
       "        7220]),\n",
       " array([  18,   73,  125,  169,  194,  214,  302,  392,  456,  468,  475,\n",
       "         483,  522,  592,  616,  651,  708,  863,  927,  931,  946,  955,\n",
       "         980,  982, 1031, 1191, 1253, 1408, 1416, 1425, 1449, 1476, 1492,\n",
       "        1497, 1531, 1618, 1642, 1668, 1742, 1753, 1819, 1839, 1872, 1882,\n",
       "        1885, 1932, 2018, 2103, 2274, 2304, 2312, 2407, 2442, 2567, 2576,\n",
       "        2753, 2886, 2917, 3082, 3373, 3425, 3445, 3740, 3799, 3971, 4010,\n",
       "        4015, 4083, 4114, 4159, 4182, 4370, 4506, 4564, 4598, 4679, 4868,\n",
       "        4872, 4890, 4946, 5019, 5109, 5125, 5262, 5298, 5363, 5461, 5508,\n",
       "        5876, 5908, 5985, 6215, 6235, 6384, 6452, 6745, 6827, 7028, 7074,\n",
       "        7092]),\n",
       " array([ 105,  132,  157,  166,  260,  305,  311,  434,  452,  490,  646,\n",
       "         761,  865, 1004, 1040, 1097, 1220, 1303, 1459, 1500, 1518, 1559,\n",
       "        1671, 1730, 1737, 1766, 1792, 1853, 1861, 2167, 2266, 2310, 2331,\n",
       "        2437, 2441, 2522, 2647, 2776, 2792, 2875, 2883, 3109, 3119, 3177,\n",
       "        3207, 3243, 3284, 3298, 3464, 3478, 3644, 3656, 3755, 3805, 3806,\n",
       "        3907, 3995, 4124, 4125, 4227, 4244, 4340, 4344, 4364, 4490, 4507,\n",
       "        4517, 4675, 4695, 4797, 4807, 4916, 4926, 5013, 5170, 5226, 5249,\n",
       "        5315, 5413, 5475, 5515, 5517, 5540, 5818, 5976, 5992, 6085, 6123,\n",
       "        6132, 6154, 6654, 6697, 6719, 6815, 6939, 6969, 7169, 7178, 7264,\n",
       "        7268]),\n",
       " array([  86,  143,  204,  230,  336,  398,  402,  438,  527,  553,  688,\n",
       "         707,  722,  740,  890,  900,  952, 1002, 1019, 1181, 1221, 1247,\n",
       "        1262, 1275, 1280, 1366, 1383, 1415, 1422, 1502, 1574, 1725, 1821,\n",
       "        1829, 1938, 2252, 2286, 2320, 2324, 2457, 2651, 2714, 2786, 2808,\n",
       "        2951, 3151, 3434, 3458, 3477, 3479, 3593, 3657, 3721, 3816, 3863,\n",
       "        3973, 3975, 3979, 4279, 4358, 4572, 4645, 4762, 4791, 4885, 4923,\n",
       "        4965, 4992, 5082, 5194, 5240, 5360, 5379, 5398, 5412, 5432, 5444,\n",
       "        5530, 5726, 5901, 6047, 6131, 6145, 6233, 6241, 6363, 6438, 6733,\n",
       "        6835, 6895, 6914, 6950, 6980, 7004, 7016, 7024, 7058, 7066, 7104,\n",
       "        7180]),\n",
       " array([  43,  160,  415,  428,  571,  601,  709,  800,  829,  845, 1091,\n",
       "        1197, 1215, 1330, 1338, 1446, 1474, 1566, 1594, 1617, 1711, 1765,\n",
       "        1826, 1914, 1944, 1955, 2072, 2186, 2342, 2484, 2488, 2588, 2672,\n",
       "        2813, 2897, 2907, 2961, 2976, 2988, 3037, 3097, 3230, 3233, 3253,\n",
       "        3261, 3317, 3612, 3675, 3687, 3724, 3759, 3837, 3946, 3967, 3972,\n",
       "        4165, 4335, 4367, 4436, 4475, 4487, 4591, 4600, 4717, 4744, 4812,\n",
       "        4825, 4925, 4930, 4959, 5002, 5009, 5089, 5142, 5200, 5252, 5494,\n",
       "        5533, 5621, 5689, 5738, 5827, 5852, 5907, 5945, 6064, 6086, 6167,\n",
       "        6402, 6441, 6464, 6480, 6575, 6688, 6775, 6832, 7335, 7359, 7457,\n",
       "        7494]),\n",
       " array([ 141,  151,  199,  212,  255,  298,  371,  387,  425,  535,  634,\n",
       "         636,  742,  907,  920, 1042, 1406, 1414, 1455, 1556, 1930, 1931,\n",
       "        2098, 2102, 2372, 2524, 2599, 2648, 2691, 2695, 2711, 2755, 2798,\n",
       "        2812, 2816, 2850, 2854, 2859, 2911, 2978, 3058, 3070, 3110, 3111,\n",
       "        3139, 3163, 3171, 3229, 3307, 3384, 3393, 3422, 3487, 3514, 3547,\n",
       "        3626, 3662, 3670, 3723, 3794, 3886, 4042, 4067, 4217, 4241, 4258,\n",
       "        4560, 4700, 4871, 4920, 4962, 4981, 4995, 5157, 5188, 5314, 5399,\n",
       "        5543, 5663, 5732, 5816, 5863, 5940, 5977, 6006, 6071, 6152, 6169,\n",
       "        6216, 6261, 6391, 6456, 6626, 6711, 6770, 7053, 7128, 7190, 7342,\n",
       "        7435]),\n",
       " array([  48,   53,  107,  137,  254,  284,  377,  382,  451,  472,  511,\n",
       "         520,  528,  559,  621,  869, 1164, 1202, 1299, 1508, 1523, 1530,\n",
       "        1563, 1690, 1834, 1840, 1881, 2094, 2248, 2280, 2285, 2388, 2469,\n",
       "        2577, 2668, 2717, 2733, 2809, 2936, 3014, 3090, 3164, 3175, 3201,\n",
       "        3274, 3432, 3531, 3552, 3566, 3592, 3849, 4093, 4262, 4303, 4316,\n",
       "        4383, 4480, 4589, 4652, 4683, 4687, 4693, 4863, 4982, 5091, 5101,\n",
       "        5171, 5270, 5317, 5502, 5613, 5674, 5741, 5784, 5787, 5797, 5800,\n",
       "        5848, 5966, 6013, 6021, 6046, 6219, 6323, 6392, 6407, 6475, 6521,\n",
       "        6560, 6611, 6639, 6656, 6691, 6721, 6734, 6927, 7258, 7274, 7285,\n",
       "        7496]),\n",
       " array([  63,   67,   89,  130,  179,  248,  296,  309,  313,  418,  519,\n",
       "         622,  769,  781,  897,  956, 1115, 1242, 1271, 1290, 1352, 1368,\n",
       "        1596, 1597, 1732, 1733, 1929, 1973, 2117, 2231, 2237, 2257, 2261,\n",
       "        2276, 2380, 2502, 2509, 2616, 2637, 2913, 2945, 3007, 3064, 3244,\n",
       "        3397, 3431, 3438, 3480, 3508, 3572, 3699, 3936, 3983, 3993, 4023,\n",
       "        4054, 4153, 4158, 4183, 4264, 4368, 4425, 4636, 4698, 4757, 4794,\n",
       "        4828, 4892, 4898, 5056, 5296, 5302, 5397, 5407, 5409, 5443, 5536,\n",
       "        5926, 6019, 6022, 6098, 6278, 6297, 6395, 6399, 6433, 6503, 6577,\n",
       "        6581, 6623, 6624, 6677, 6678, 6808, 6836, 7057, 7088, 7237, 7430,\n",
       "        7432]),\n",
       " array([  65,   74,   97,  192,  322,  499,  516,  574,  629,  848,  892,\n",
       "         969, 1029, 1186, 1229, 1301, 1350, 1629, 1655, 1922, 1999, 2004,\n",
       "        2019, 2088, 2161, 2292, 2298, 2318, 2514, 2554, 2563, 2680, 2751,\n",
       "        2825, 2849, 3054, 3056, 3308, 3382, 3430, 3454, 3496, 3649, 3722,\n",
       "        3828, 3906, 3916, 3955, 4029, 4164, 4362, 4434, 4481, 4527, 4540,\n",
       "        4542, 4582, 4601, 4800, 4804, 4857, 4903, 4991, 5036, 5054, 5079,\n",
       "        5222, 5279, 5392, 5420, 5531, 5537, 6010, 6027, 6125, 6151, 6211,\n",
       "        6304, 6331, 6394, 6411, 6470, 6619, 6813, 6817, 6957, 6962, 6964,\n",
       "        7069, 7083, 7201, 7221, 7269, 7290, 7299, 7391, 7396, 7399, 7400,\n",
       "        7440])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches = []\n",
    "for train, test in kf.split(X_train, y_train):\n",
    "    batches.append(test)\n",
    "batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        for batch in batches:\n",
    "            X_batch = X_train[batch]\n",
    "            y_batch = y_train[batch]\n",
    "            sess.run(training_run, feed_dict={X: X_batch, y: y_batch})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What are youre actual predicted classes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is your accuracy score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about your confusion matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network for Multi-Class Classification\n",
    "\n",
    "---\n",
    "\n",
    "In _multi-class_ classification (i.e., more than 2 classes), we typically setup on output unit **per class** and use a \"softmax\" activation function to normalize all values between 0 and 1 so that they look like probabilities.  The output unit with the largest value corresponds to the class prediction.  (I.e., unit 3 had the highest value, so I will predict class 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, (None, 4))\n",
    "y = tf.placeholder(tf.float32, (None, 3))\n",
    "\n",
    "h1 = tf.layers.dense(X, 4, tf.nn.relu)\n",
    "y_hat = tf.layers.dense(h1, 3)\n",
    "\n",
    "loss = tf.losses.softmax_cross_entropy(y, y_hat)\n",
    "optimizer = tf.train.AdadeltaOptimizer(.01)\n",
    "training_run = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        sess.run(training_run, feed_dict={X: X_train, y: y_train})\n",
    "    \n",
    "    pred = sess.run(y_hat, feed_dict={X: X_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = pred.argmax(axis=1) # Predictions on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23684210526315788"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test.argmax(axis=1), classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Suggestions\n",
    "\n",
    "---\n",
    "\n",
    "- In binary classification, start with a single output node with a sigmoid activation function.\n",
    "- For multi-class classification, have one output node for each class and use the softmax activation function.\n",
    "- For hidden layers, the ReLU and hyperbolic tangent (tanh) activation functions often work well.  Start with the ReLU as your first trial.\n",
    "- Start with one hidden layer, then trying adding another if performance is not good.\n",
    "- For simplicity, start with the same number of units in each hidden layer, then increase this number for all hidden layers simultaneously if the performance is not good.\n",
    "- Alternatively, you can use the \"stretchy pants\" approach and through a lot of hidden nodes into your network, but then stop training as soon as you detect the onset of overfitting.\n",
    "\n",
    "> These suggestions are largely adapted from the book _Hands-on Machine Learning with Scikit-Learn & Tensorflow_ by Aurelien Geron, a book I highly recommend!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
